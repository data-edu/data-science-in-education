# Walkthrough 1: The education data science pipeline with online science class data {#c07}

## Topics Emphasized

In this and future walkthroughs, we focus on the distinct but interconnected steps of the data science process, based on the six steps outlined in R for Data Science by @r4ds2023 (see [Chapter 3](#c03)). Each chapter emphasize specific topics while incorporating elements of the entire process.

This chapter on the education data science pipeline emphasizes:

- Tidying data 
- Transforming data
- Visualizing data
- Modeling data

## Functions Introduced

- `across()`
- `data.frame()`
- `dplyr::summarize()`
- `group_by()`
- `tibble()`
- `tidyr::pivot_longer() and tidyr::pivot_wider()`
- `tidyr::left_join()`, `tidyr::right_join()`, `tidyr::semi_join()`, and `tidyr::anti_join()`
- `mutate()`
- `summarize()`
- `lm()`
- `ggplot2::ggplot()`
- `apaTables::apa.cor.table()`
- `sjPlot::tab_model()`

## Vocabulary

In this walkthrough sections, we include key terms introduced in the chapter:

  - data frame
  - item
  - joins
  - keys
  - log-trace data
  - passed arguments
  - reverse scale
  - regression
  - survey
  - tibble
  - tidy selection

## Introduction to the Walkthroughs

This chapter is the first of eight walkthroughs included in the book, each exploring the *education data science pipeline* through different datasets. The pipeline encompasses common steps in data science projects, such as cleaning, tidying, exploring, visualizing, and modeling data, as depicted by @r4ds2023.

```{r fig7-1, fig.cap = "Data Science Cycle", echo = FALSE, fig.alt="A diagram displaying the data science cycle: Import -> Tidy -> Understand  (which has the phases Transform -> Visualize -> Model in a cycle) -> Communicate. Surrounding all of these is Program  Import, Tidy, Transform, and Visualize is highlighted."}
knitr::include_graphics("./man/figures/Figure 7.1.png")
```

Using data from a number of sources, we show the end-to-end process of working with an education dataset. While the walkthrough topics vary, their structure and section headings remain consistent. Each walkthrough begins with a vocabulary section, followed by an introduction to the dataset and the central question or problem.

This chapter assumes familiarity with four core concepts that comprise the foundational skills framework: projects, functions, packages, and data. If you would 
like a refresher about (or an introduction to) any of those, refer to the previous chapter, [Chapter 6](#06).

## Chapter Overview

In this walkthrough, we explore some of the key steps that are a part of many data science projects in education. In particular, we focus on how to tidy and transform data. These steps are sometimes referred to as "data wrangling" or "data manipulation". These tasks rely heavily on a set of tools that we use throughout *all* of the walkthroughs, those associated with the {tidyverse}, which is a set of packages for data manipulation, exploration, and visualization. 

The {tidyverse} follows the design philosophy of 'tidy' data [@wickham2014]. Tidy data has a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. We'll discuss the {tidyverse} and tidy data throughout the book. For more information, see the Foundational Skills chapters or <https://www.tidyverse.org/>.

### Background

The online science classes we explore in this chapter were created by instructors through a statewide online course provider designed to supplement - but not replace - students' enrollment in their local schools. For example, students may have chosen to enroll in an online physics class if their school did not offer one. The data were originally collected for a research study using multiple data sources to understand students' course-related motivation. These datasets include:

1.  A self-report survey assessing three aspects of students' motivation
2.  Log-trace data, such as data output from the learning management system (LMS)
3.  Discussion board data
4.  Academic achievement data

Our primary *purpose* of this walkthrough is to analyze factors that explain students' performance in these online courses. The *problem* is a very common challenge when it comes to data science in education: the data are complex and require significant processing before analysis We will revisit this dataset in the final walkthrough [Walkthrough 8/Chapter 14](#c14), where we will provide more details on the dataset and the context. 

To understand students' performance, we will focus on an LMS variable that represents the amount of time students spent in the LMS. we will explore how the type of science course and class section influence performance.

We begin by describing how these data sources were provided by the school.

### Data Sources

#### Data Source \#1: Self-Report Survey about Students' Motivation

The first data source is a self-report survey conducted before the course began. The survey included ten items, each corresponding to one of three motivation *measures*: interest, utility value, and perceived competence. A *measure* is a concept that we try to make concrete and assessable using survey questions. The three motivation measures are from Expectancy-Value Theory, which states that students are motivated to learn when
they both believe that they can succeed (expectancy, also known as
perceived competence) and believe that the concept they are trying to learn is
important (value) [@wigfield2000]. There are multiple types of value, but we
examine two of them here: interest and utility value. Interest is how engaging or enjoyable students find the subject,  and utility value is how relevant students believe the subject is to their future. This survey included the following ten items:

1.  I think this course is an interesting subject. (Interest)
2.  What I am learning in this class is relevant to my life. (Utility value)
3.  I consider this topic to be one of my best subjects. (Perceived competence)
4.  I am not interested in this course. (Interest - reverse coded)
5.  I think I will like learning about this topic. (Interest)
6.  I think what we are studying in this course is useful for me to know.
    (Utility value)
7.  I don’t feel comfortable when it comes to answering questions in this area.
    (Perceived competence - reverse coded)
8.  I think this subject is interesting. (Interest)
9.  I find the content of this course to be personally meaningful. (Utility
    value)
10. I’ve always wanted to learn more about this subject. (Interest)

### Data Source \#2: Log-Trace Data

*Log-trace data* refers to data generated from interactions with digital
technologies, such as social media posts (see [Chapter 11](#c11) and [Chapter 12](#c12) for walkthroughs with social media posts). In education, an increasingly common source is data from LMSs and other digital tools [@siemens2012]. For this walkthrough, we use a summary form of log-trace data: the number of minutes students spent on the course. While this data is relatively simple, more complex sources of log-trace data can include detailed timestamps, such as when students started and stopped accessing the course.

### Data Source \#3: Academic Achievement and Gradebook Data

A common source of data in the education realm  is students' graded assignments. In this walkthrough, we examine only students' final course grade.

### Data Source \#4: Discussion Board Data

Discussion board data is rich but unstructured because it consists of large text blocks written by students wrote. Although discussion board data was collected for this research project, we do not examine it in this walkthrough. More information about analyzing text data can be found in [Chapter 11](#c11).

### Methods

In this walkthrough, we focus on merging datasets using the different joins available in the {dplyr} package and begin exploring how to run linear models in R. 

## Load Packages

This analysis uses R packages, which are collections of R code that help users code more efficiently, as discussed in [Chapter 1](#c1). We load these packages with the function `library()`. The specific packages used here help us structure the data with the {tidyverse} [@R-tidyverse], create formatted tables with {apaTables} [@R-apaTables] and
{sjPlot} [@R-sjPlot], and export datasets with {readxl} [@R-readxl].

***Install packages (if necessary)***

If you haven't installed these packages yet, you need to do so before loading them. If you run the code below *before* installing them, you will see a message indicating that the package is not available. If they're already installed, you can skip this step.

You can install a single package, such as the {tidyverse} package, as follows:

```{r, eval = F}
install.packages("tidyverse")
```

If you must install two or more packages, you can do so in a single call to the `install.packages()` function. Supply the names of the packages as a vector using `c()`:

```{r, eval = F}
install.packages(c("tidyverse", "apaTables"))
```

Once installed, you only need to load a package with `library()` for future use. For more on the installation of packages, see the [Packages section](#c06p) of [Chapter 6](#c06). 

``` {r, loading-packages, message = F, warning = F}
library(tidyverse)
library(apaTables)
library(sjPlot)
library(readxl)
library(dataedu)
```

## Import Data

This code chunk loads the log-trace data and self-report survey data from the {dataedu} package. Each dataset is assigned to an object using `<-`, with a unique name for each dataset.

```{r}
# Pre-survey for the F15 and S16 semesters
pre_survey <- dataedu::pre_survey

# Gradebook and log-trace data for F15 and S16 semesters
course_data <- dataedu::course_data

# Log-trace data for F15 and S16 semesters - this is for time spent in the LMS
course_minutes <- dataedu::course_minutes
```

## View Data

Now that we've successfully loaded all three log-trace datasets, we can visually inspect the data by typing their assigned names. Running each line individually will display the first few rows of each dataset.

```{r}
pre_survey
course_data
course_minutes
```

## Process Data

Survey data often requires processing to be useful. We begin with the pre-survey dataset and transform the self-report items into three scales: 1) interest, 2) perceived competence, and 3) utility value. This involves:

  - Renaming question variables for clarity
  - Reversing the response scales for questions 4 and 7
  - Categorizing each question into a measure
  - Computing the mean of each measure

Let's take these steps in order:

1.  Save the pre-survey data as a new object with the same name, "pre\_survey." Rename the question columns to something much simpler using the `rename()` function by using this format: `new_name = old_name`. Use the {dplyr} functions `mutate()` and `across()` (explained below) for further transformations.

```{r}
pre_survey  <-
  pre_survey  %>%
  # Rename the questions something easier to work with because R is case sensitive
  # and working with variable names in mix case is prone to error
  rename(
    q1 = Q1MaincellgroupRow1,
    q2 = Q1MaincellgroupRow2,
    q3 = Q1MaincellgroupRow3,
    q4 = Q1MaincellgroupRow4,
    q5 = Q1MaincellgroupRow5,
    q6 = Q1MaincellgroupRow6,
    q7 = Q1MaincellgroupRow7,
    q8 = Q1MaincellgroupRow8,
    q9 = Q1MaincellgroupRow9,
    q10 = Q1MaincellgroupRow10
  ) %>%
  # Convert all question responses to numeric
  mutate(across(q1:q10, as.numeric))
```

Let's take a moment to discuss `mutate` and `across()`. The `mutate()` function modifies the values in an existing column or creates new columns. It's useful in education datasets because you'll often need to create new variables before analysis. To learn a little more about `mutate()`, let's demonstrate how to create and modify a data frame called "df". 

A data frame is a two-dimensional structure that stores tables. The table has a header and data rows, where each cell stores a value. The example uses the `tibble()` function from the {tibble} package. The function creates a tibble, which is special type of data frame that makes working with the tidy data a little easier. More information is available in R for Data Science [@r4ds2023].

We fill this data frame with two columns: "male" and "female." Each column has only one value, which is set to 5. In the second part of the code, we create a `total_students` column by adding the number of `male` students and `female` students using `mutate()`.

```{r}
# Dataset of students
df <- tibble(male = 5, female = 5)

# Use mutate to create a new column called "total_students" 
# Populate that column with the sum of the "male" and "female" variables
df %>% mutate(total_students = male + female)
```

The `across()` function within `mutate()` applies a transformation to multiple columns at once. It follows the format, `mutate(across(variables, transformation))`. For example, in our `df` dataset, we can use `across()` to multiply the male column by 2. Note that because we did not save `df` in the previous code chunk to an object, the `total_students` column is no longer present:

```{r}
df %>% mutate(across(male, ~ . * 2))
```
The `~ . * 2` syntax is a shorthand for applying a function to each selected column. Let's apply another transformation using `across()` where we square the `female` column. Note that because we did not save `df` in the previous code chunk to an object, `male` is still set to 5:

```{r}
df %>% mutate(across(female, ~ . ^ 2))
```

Because of something called tidy selection, we don’t have to list each variable explicitly.  In the `pre_survey` code above, `q1:q10` selects all consecutive columns from `q1` to `q10` using `:` instead of listing each one. The `as.numeric()` function is applied to these variables to transform them to a numeric format.

Other useful selection helpers include `c()` for combining selections, `starts_with()` for selecting variables by prefix, `matches()` for pattern matching with regular expressions, and so on. We'll explore more examples in future walkthroughs.

2.  Next, we'll reverse the scale of the survey responses on questions 4 and 7 so that all responses can be interpreted in the same way. As you can see from the survey questions we listed earlier in the chapter, the phrasing of questions 4 and 7 is opposite to the others.

Education datasets often use numerical codes to describe demographics, such as disability categories, race groups, or test proficiency levels. In such cases, it's useful to replace these codes with more descriptive labels. You can use the `case_when()` function when you need to modify the values in a column based on some criteria. 

For a example, a consultant analyzing state test results might use `case_when()` to replace proficiency codes like `1`, `2`, or `3` to labels like "below proficiency", "proficient", or "advanced." In our case, we’ll use `case_when()` within a function to reverse the scale of item responses.
	
`case_when()` works by evaluating each value in a column against a sequence of conditions. It is helpful because, instead of writing complex loops, you can apply multiple conditions in a compact and readable way.

The left-hand side of each `case_when()` condition is a logical statement that returns `TRUE` or `FALSE`. If the condition is met, the corresponding values are replaced with the right-hand side.

Let's revisit our `df` tibble.  Below, we use `case_when()` within `mutate()` to modify the `male` column. The condition `male == 5 ~ 10` means that whenever `male` equals (`==`) `5`, it will be replaced with `10`:

```{r}
df  %>%
    mutate(male = case_when(male == 5 ~ 10))
```

Here are some logical operators you can use in the future:

  - `==`: equal to
  - `>`: greater than
  - `<`: lesser than
  - `>=`: greater than or equal to
  - `<=`: lesser than or equal to
  - `!=`: not equal to
  - `!`: not
  - `&`: and
  - `|`: or
 
Returning to the `pre_survey` dataset, let's reverse the scale for question 4 using `case_when()`. We’ll recode the values so that `1` becomes `5`, `2` becomes `4`, and so on. The `.default = NA` argument makes it so  any values not explicitly matched will be assigned `NA`.

```{r}
pre_survey <-
    pre_survey %>%
    mutate(
        case_when(
            q4 == 1 ~ 5,
            q4 == 2 ~ 4,
            q4 == 3 ~ 3,
            q4 == 4 ~ 2,
            q4 == 5 ~ 1,
            .default = NA
            )
    )
```

This approach works well, but you can imagine it'd become quite tedious if we had to rewrite the same `case_when()` for multiple questions. Rather than duplicating code for questions 4 and 7, we'll create a function that reverses the scale for any specified questions. This results in more concise code, and it will be easier to maintain.

Let's make this all concrete. In the first part of the code chunk below, we define our custom function. Note that running this code won't modify the dataset yet. Instead, it creates a reusable, general-purpose function that we can apply to the specific survey questions we want to recode. If you need a refresher on writing custom functions, see the Writing Your Own Functions section in [Chapter 6](#c6).

```{r}
# This part of the code is where we write the function:
# Function for reversing scales 
reverse_scale <- function(question) {
  # Reverses the response scales for consistency
  #   Arguments:
  #     question - survey question
  #   Returns: 
  #    a numeric converted response
  # Note: even though 3 is not transformed, case_when expects a match for all
  # possible conditions, so it's best practice to label each possible input
  # and use .default = as the final statement returning NA for unexpected inputs
  x <- case_when(
      question == 1 ~ 5,
      question == 2 ~ 4,
      question == 3 ~ 3, 
      question == 4 ~ 2,
      question == 5 ~ 1,
      .default = NA
  )
  x
}
```

Now, we apply our `reverse_scale()` function to adjust the scales for questions 4 and 7. Using the pipe operator (`%>%`), we pass our dataset into `mutate()`, where we call `reverse_scale()` to transform the selected questions. By using `mutate()`, we replace the existing values in questions 4 and 7 with their newly recoded versions.

```{r}
pre_survey <-
  pre_survey %>%
  mutate(q4 = reverse_scale(q4),
         q7 = reverse_scale(q7))
```

3.  Next, we'll use the `pivot_longer()` function to reshape the `pre_survey` dataset from wide to long format. That means instead of having `r scales::comma(pre_survey |> nrow())` observations of `r scales::comma(pre_survey |> ncol() - 1)` variables, we will now have 11,020 observations of 4 variables. 

By using `pivot_longer()`, we make it so that each question-response pair gets its own row. Since the dataset contains 10 question variables (columns), we end up with 10 times as many observations (rows) as before. Additionally, we no longer need a separate column for each individual question, since each question-response pair is now on its own row. What was previously one row of data now takes up ten rows of data. so `pivot_longer()` automatically removes the redundant columns. We'll save the pivoted dataset as an object called `measure_mean`.

Using the tidy selector `:`, we specify the range of columns to pivot (`q1` to `q10`). he `names_to` argument creates a new column to store the original column names (`q1` through `q10`), while the `values_to` argument contains their corresponding values.

``` {r}
measure_mean <-
    pre_survey %>%
    pivot_longer(cols = q1:q10,
                 names_to = "question",
                 values_to = "response")
```

4.  Next, we'll take our new `measure_mean` dataset and create a new column called `measure`. We'll fill that column with one of three categories for each question:

  - `int`: Interest
  - `uv`: Utility value
  - `pc`: Perceived competence

We will use the `case_when()` function we learned earlier in order to assign the categories. When we pivoted the data from wide to long format, all question numbers (`q1`, `q2`, etc.) were stored in a single column. Now, we want to map the question numbers to their respective categories.

We'll introduce a new operator in order to do this: `%in%`. This operator checks whether a value exists to a specified list. In the code below, we assign:

* Questions 1, 4, 5, 8, and 10 to `int`. 
* Questions 2, 6, and 9 to `uv` 
* Questions 3 and 7 to `pc`. 

We define each list using `c()`. For example, `c("q3", "q7")` contains question 3 and 7. Then, we create a new `measure` column containing the assigned categories.

```{r}
# Add measure variable 
measure_mean <- measure_mean %>% 
  mutate(
    measure = case_when(
      question %in% c("q1", "q4", "q5", "q8", "q10") ~ "int",
      question %in% c("q2", "q6", "q9") ~ "uv",
      question %in% c("q3", "q7") ~ "pc",
      .default = NA)
  )
```

5. Finally, we'll take that same `measure_mean` dataset to create a new variable called `mean_response`. To calculate the mean by category, we first need to group the data by category using the `group_by()` function. This function prepares us for grouped calculations. 

Next, we'll use the `summarize()` function to create two new variables.

* `mean_response`: The mean response for each category, calculated using the `mean()` function.
* `percent_NA`: The percentage of missing values for each category.

We use `summarize()` instead of `mutate()` because we want to condense data into summary statistics instead of modifying values in each row. The` na.rm = TRUE` argument in `mean()` ignores `NA` values when calculating the mean so they do not return an `NA` result.

``` {r}
measure_mean <- measure_mean %>%
    group_by(measure) %>%
    summarize(mean_response = mean(response, na.rm = TRUE),
              percent_NA = mean(is.na(response)))

measure_mean
```

With that final step, we have finished processing the `pre_survey` dataset. We renamed the key variables and computed means for future use.

### Processing the Course Data

When we loaded three datasetes earlier in the chapter: `pre_survey`, `course_data`, and `course_minutes.` Next, we can process the course data that we already loaded into the environment in order to create new variables which we can use in analyses.

Information about the course subject, semester, and section are stored in a single column, `CourseSectionOrigID`. This format of data storage is not ideal. If we instead give each piece of information its own column, we'll have more opportunities for later analysis. We'll use a function called `separate()` to do this. Below, we will load `course_data` and run the `separate()` function to split up the subject, semester, and section so we can use them later on. 

``` {r}
# split course section into components
course_data <- 
  course_data %>%
  # Give course subject, semester, and section their own columns
  separate(
    col = CourseSectionOrigID,
    into = c("subject", "semester", "section"),
    sep = "-",
    remove = FALSE
  )
```

After running the code chunk above, take a look at the dataset `course_data` to
make sure it looks the way you'd expect it to look. In this case, we are
expecting that we will add 3 new variables, taking the total number of variables
in this dataset from 8 to 11. We will still see the original variable
`CourseSectionOrigID` in the data as well.

### Joining the Data

In this chapter, we are looking at two datasets that are derived from the same
courses. In order for these datasets to be most useful to us, we'd like all that
data to be in one place.

To join the course data and pre-survey data, we need to create similar *keys*.
Our goal here is to have one variable that matches across both datasets. Once we
have that common variable in both datasets, we can merge the datasets on the
basis of that variable.

When we look at the `course_data` and `pre_survey` datasets in our environment,
we see that both have variables for the course and the student. However, this
information is captured in different variable names in each dataset. Our first
goal will be to rename two variables in each of our datasets so that they will
match. One variable will correspond to the course, and the other will correspond
to the student. We are not changing anything in the data itself at this step -
instead, we are just cleaning the column headers up so that we can look at the
data all in one place.

Let's start with the pre-survey data. We will rename `RespondentID` and
`opdata_CourseID` to be `student_id` and `course_id`, respectively. Here, we are
going to use the same `rename()` function we learned earlier in this chapter.

```{r}
pre_survey <-
  pre_survey %>%
  rename(student_id = opdata_username,
         course_id = opdata_CourseID)

pre_survey
```

Those variable names look better now!

When we look at the data more closely, though, we will notice that the
`student_id` variable has another issue - the variable has some additional
characters before and after *the actual ID* that we will need to be able to join
this data with the other data sources we have. Why does this variable have these
additional characters? Why is there a "1" at the end of every 5-digit ID number?
We are not sure! Sometimes, educational data from different systems (used for
different purposes) may have additional "meta"-data added on. In any event, here
is what the variables look like before processing:

```{r}
head(pre_survey$student_id)
```

What we need is the five characters in between the underscore symbols - these:
`_`.

One way to do this is to use the `str_sub()` function from the {stringr}
package. This function lets us subset *string* variables: variables that store
text data. You can specify the indices of the variables you want the string to
*start* and *end* with.

Here, for example, is how we can select only the content starting with the
second character, skipping the first underscore in the process. This next chunk
of code will not change our data, but will show you how the `str_sub()` function
works by supplying a number-and-underscore combination to the function that is
in the same format as our data.

```{r}
str_sub("_99888_1", start = 2)
```

We can apply the same thinking to delete characters from the end of a string. We
will use a `-` to indicate that we want to start from the right side of the
string of characters. Interestingly, when we specify the argument `end` below,
we will tell it the placement of the first character we want to INCLUDE. When we
type `end = -3`, we end up deleting only the last 2 characters. Our new
rightmost character will be the final 8.

```{r}
str_sub("_99888_1", end = -3)
```

Putting the pieces together, the following should return what we want. Try
running the code below to see if it yields the 5-digit ID number we are shooting
to extract.

```{r}
str_sub("_99888_1", start = 2, end = -3)
```

_Note: you may receive a warning telling you that `NA` values were introduced by coercion. This happens when we change data types, and we will overlook this warning message for the purposes of this walkthrough._

We can apply this process to our data using `mutate()`. We convert the string
into a number using `as.numeric()` in the next portion of the code. This step is
important so the data can be joined to the other, numeric `student_id` variables
(in the other datasets):

```{r}
# Re-create the variable "student_id" so that it excludes the extraneous characters
pre_survey <- pre_survey %>% 
  mutate(student_id = str_sub(student_id, start = 2, end = -3))

# Save the new variable as numeric so that R no longer thinks it is text 
pre_survey <- pre_survey %>% 
  mutate(student_id = as.numeric(student_id))

```

Now that the `student_id` and `course_id` variables are ready to go in the
`pre_survey` dataset, let's proceed to the course data. Our goal is to rename
two variables that correspond to the course and the student so that we can match
with the other variables we just created for the pre-survey data. In the code
chunk below, we will rename both those variables.

``` {r}
course_data <-
  course_data %>%
  rename(student_id = Bb_UserPK,
         course_id = CourseSectionOrigID)
```

Now that we have two variables that are consistent across both datasets - we
have called them `course_id` and `student_id` - we can join the two datasets
using the {dplyr} function, `left_join()`.

`left_join()` is named based on the 'direction' that the data is being joined.
Note the order of the data frames passed to our "left" join. Left joins retain
all of the rows in the data frame on the "left", and joins every matching row in
the right data frame to it. We will use two variables as keys for joining the
datasets, and we will specify those after the word `by`.

Let's save our joined data as a new object called `dat`.

```{r}
dat <-
  left_join(course_data, pre_survey,
            by = c("student_id", "course_id"))
dat
```

Let's hone in on how this code is structured. After `left_join()`, we see
`course_data` and then `pre_survey`. In this case, `course_data` is the "left"
data frame (passed as the *first* argument), while `pre_survey` is the "right"
data frame (passed as the *second* argument). So, in the above code, what
happens? You can run the code yourself to check.

Our aim with that code is that all of the rows in `course_data` are retained in
our new data frame, `dat`, with matching rows of `pre_survey` joined to it. An
important note is that there are not multiple matching rows of `pre_survey`;
otherwise, you would end up with more rows in `dat` than expected. There is a
lot packed into this one function. Joins are extremely powerful - and common -
in many data analysis processing pipelines, both in education and in any field.
Think of all of the times you have data in more than one data frame, but you
want everything to be in a single data frame! As a result, we think that joins
are well worth investing the time to be able to use.

With most types of data, `left_join()` is helpful for carrying out most tasks
related to joining datasets. However, there are functions for other types of
joins that we want to make sure you know how to use. They may be less frequently
used than `left_join()`, but they are still worth mentioning. Note that for all
of these, the "left" data frame is always the first argument, and the "right"
data frame is always the second. When running the code chunks below, it can be
helpful to pay attention to the number of observations and variables in the
datasets before and after the joining. Eventually, the obscure names of these
types of joins will start to become more intuitive as you use them more often.

#### `semi_join()`

`semi_join()`: joins and retains all of the *matching* rows in the "left" and
"right" data frame. This is useful when you are only interested in keeping the
rows (or cases/observations) that are able to be joined. `semi_join()` will not
create duplicate rows of the left data frame, even when it finds multiple
matches on the right data frame. It will also keep only the columns from the
left data frame.

For example, the following returns only the rows that are present in both
`course_data` and `pre_survey`:

```{r}
dat_semi <- 
  semi_join(course_data,
            pre_survey,
            by = c("student_id", "course_id"))

dat_semi
```

#### `anti_join()`

`anti_join()`: *removes* all of the rows in the "left" data frame that can be
joined with those in the "right" data frame.

```{r}
dat_anti <-
  anti_join(course_data,
            pre_survey,
            by = c("student_id", "course_id"))

dat_anti
```

#### `right_join()`

`right_join()`: perhaps the least helpful of the three, `right_join()` works the
same as `left_join()`, but by retaining all of the rows in the "right" data
frame, and joining matching rows in the "left" data frame (so, the opposite of
`left_join()`).

```{r}
dat_right <-
  right_join(course_data,
             pre_survey,
             by = c("student_id", "course_id"))

dat_right
```

If we wanted this to return exactly the same output as `left_join()` (and so to
create a data frame that is identical to the `dat` data frame we created above), we could
simply switch the order of the two data frames to be the opposite of those used
for the `left_join()` above:

```{r}
dat_right <-
  right_join(pre_survey,
            course_data,
            by = c("student_id", "course_id"))

dat_right
```

Now that we've gone through the different types of joins available, we will
return to our main focus: joining our course datasets together. While we didn't
do any data processing steps on it, we still have the `course_minutes` dataset
in our environment from when we loaded it there earlier in the chapter. In the
code chunk below, we will rename the necessary variables in that dataset so that
it is ready to merge. Then, we will merge the `course_minutes` dataset, with its
newly renamed variables `student_id` and `course_id`, with our `dat` dataset.

```{r}
course_minutes <-
  course_minutes %>%
  rename(student_id = Bb_UserPK,
         course_id = CourseSectionOrigID)

course_minutes <-
  course_minutes %>%
  # Change the data type for student_id in course_minutes so we can match to 
  # student_id in dat
  mutate(student_id = as.integer(student_id))

dat <- 
  dat %>% 
  left_join(course_minutes, 
            by = c("student_id", "course_id"))
```

Note that they're now combined, even though the course data had many more rows.
The pre-survey data has been joined for each student-course combination. We
have a pretty large data frame! Let's take a quick look.

``` {r}
dat
```

It looks like we have `r nrow(dat)` observations from 22 variables.

### Finding Distinct Cases at the Student-Level

If a student was enrolled in two courses, she will have a different final grade for each of
those two courses. However,  our data in
its current form has many rows representing *each course. An easy way we can visually inspect to make sure every row
is the same for the same student, by course is to use the `glimpse()` function. 
Try it below.

```{r}
glimpse(dat)
```

You can also use `View(dat)` in order to view the data in RStudio's viewer.

Visually inspecting the first several rows of data, we see that they all
correspond to the same student for the same course. As we expected, the
`FinalGradeCEMs` variable (representing students' final grade) is also consistent across these rows. 

Since we are not carrying out a finer-grained analysis using the `Gradebook_Item`, these
duplicate rows are not necessary. We only want variables at the student level,
and not at the level of different gradebook items. We can extract only the
unique student-level data using the `distinct()` function. This function takes
as arguments the name of the data frame and the name of the variables used to
determine what counts as a unique case.

Imagine having a bucket of Halloween candy that has 100 pieces of candy. You
know that these 100 pieces are really just a bunch of duplicate pieces from a
relatively short list of candy brands. `distinct()` takes that bucket of 100
pieces and returns a bucket containing only one of each distinct piece.

Another thing to note about `distinct()` is that it will only return the
variable(s) (it is possible to pass more than one variable to `distinct()`) you
used to determine uniqueness, *unless* you include the argument `.keep_all =
TRUE`. For the sake of making it simple to view the output, we will omit this
argument for now.

Were we to run `distinct(dat, Gradebook_Item)`, what do you think would be
returned? Running the following code returns a one-column data frame that lists the names of
every distinct gradebook item.
 
``` {r}
distinct(dat, Gradebook_Item)
```

You might be wondering whether some gradebook
items have the same names across courses. We can return the unique *combination*
of courses and gradebook items by simply adding another variable to
`distinct()`:

``` {r}
distinct(dat, course_id, Gradebook_Item)
```

The data frame we get when we run the code chunk above yields a much longer (more
observations) dataset. Thus, it looks like *a lot* of gradebook items were
repeated across courses - likely across the different sections of the same
course. If you'd like, you can continue to investigate this: we would be curious
to hear what you find if you do!

Next, let's use a similar process to find the unique values at the student
level. Thus, instead of exploring unique gradebook items, we will explore unique
students (still accounting for the course, as students could enroll in more than
one course.) This time, we will add the `keep_all = TRUE` argument.

```{r}
dat <-
  distinct(dat, course_id, student_id, .keep_all = TRUE)
```

This is a much smaller data frame - with one row for each student in the course.
Whereas our prior version of the `dat` dataset had over 40,000 rows, that prior
version is only helpful if we wanted to do an analysis at the level of specific
students' grades for specific gradebook items. Our new dataset keeps only the
unique combinations of student and course, leaving us with a more manageable
number of observations: `r nrow(dat)`. Now that our data are ready to go, we can
start to ask some questions of the data.

Let's take one last step. Since we will be using the final grade variable in 
many of the figures and analyses that follow, let's rename it using the {dplyr} 
`rename()` function to something that is a bit easier to type and remember than
`FinalGradeCEMS`:

```{r}
dat <- rename(dat, final_grade = FinalGradeCEMS)
```

## Analysis

In this section, we focus on some initial analyses in the form of visualizations
and some models. We expand on these in [Chapter 13](#c13). Before we start
visualizing relationships between variables in our survey dataset, let's
introduce {ggplot2}, a visualization package we'll be using in our walkthroughs.

### About {ggplot2}

{ggplot2} is a package we’ll be using a lot for graphing our education datasets.
{ggplot2} is designed to build graphs layer by layer, where each layer is a
building block for your graph. Making graphs in layers is useful because we can
think of building up our graphs in separate parts: the data comes first, then
the x-axis and y-axis, and finally other components like text labels and graph
shapes. When something goes wrong and your ggplot2 code returns an error, you
can learn about what’s happening by removing one layer at a time and running it
again until the code works properly. Once you know which line is causing the
problem, you can focus on fixing it.

The first two lines of {ggplot2} code look similar for most graphs. The first
line tells R which dataset to graph and which columns the x-axis and y-axis will
represent. The second line tells R which shape to use when drawing the graph.
You can tell R which shape to use in your graphs with a family of {ggplot2}
functions that start with `geom_`. {ggplot2} has many graph shapes you can use,
including points, bars, lines, and boxplots. Here’s a {ggplot2} example using a
dataset of school mean test scores to graph a bar chart:

```{r fig7-1, warning = FALSE, message = FALSE, results = 'hide', fig.showtext = TRUE, fig.cap = "Example Plot"}
# make dataset
students <- 
  tibble(
    school_id = c("a", "b", "c"), 
    mean_score = c(10, 20, 30)
  )

# tell R which dataset to plot and which columns the x-axis and y-axis will represent
students %>% 
  ggplot(aes(x = school_id, y = mean_score)) + 
  # draw the plot
  geom_bar(stat = "identity",
           fill = dataedu_colors("darkblue")) +
  theme_dataedu()
```

The `data` argument in the first line tells R we’ll be using the dataset called
`students`. The `aes` argument tells R we’ll be using values from the
`school_id` column for the x-axis and values from the `mean_score` column for
the y-axis. In the second line, the `geom_bar` function tells R we’ll drawing
the graph using the bar chart format. Each line of {ggplot2} code is connected by a
`+` at the end to tell R the next line of code is an additional {ggplot2} layer to
add.

Writing code is like writing essays. There's a range of acceptable styles and certainly you can practice unusual ways of writing, but other people will find it harder to understand what you want to say. In this book, you'll see variations in {ggplot2} style, but all within what we believe is the range of acceptable conventions. Here are some examples: 

 - Piping data to `ggplot()` using `%>%` vs including it as an argument in `ggplot()` 
 - Using `ggtitle()` for labels vs using `labs()` 
 - Order of `ggplot()` levels 

It's ok if those terms are new to you. The main point is there are multiple ways to make the plot you want. You'll see that in this book and in other peoples' code. As you learn, we encourage you to practice empathy and think about how well your code conveys your ideas to other people, including yourself when you look at it many weeks from when you wrote it. 

### The Relationship between Time Spent on Course and Final Grade

One thing we might be wondering is how time spent on course is related to
students' final grade. Let's make a plot to depict that relationship. Below,
we'll use `geom_point` instead of `geom_bar`.

``` {r fig7-2, message = FALSE, warning = FALSE, results = 'hide', fig.cap = "Percentage Earned vs. Time Spent", fig.showtext = TRUE}
dat %>%
  # aes() tells ggplot2 what variables to map to what feature of a plot
  # Here we map variables to the x- and y-axis
  ggplot(aes(x = TimeSpent, y = final_grade)) + 
  # Creates a point with x- and y-axis coordinates specified above
  geom_point(color = dataedu_colors("green")) + 
  theme_dataedu() +
  labs(x = "Time Spent",
       y = "Final Grade")
```

_Note: you may receive a warning that reads `Warning message: Removed 5 rows containing missing values (geom_point).` This is due to the `NA` values that were introduced through coercion earlier in this walkthrough, and are not a cause for alarm!_

There appears to be *some* relationship. What if we added a line of best fit - a
linear model? The code below is the same plot we just made, but it includes the
addition of another layer called `geom_smooth`. 

``` {r fig7-3, message = FALSE, warning = FALSE, results = 'hide', fig.cap = "Adding a Line of Best Fit", fig.showtext = TRUE}
dat %>%
  ggplot(aes(x = TimeSpent, y = final_grade)) +
    geom_point(color = dataedu_colors("green")) + # same as above
  # this adds a line of best fit
  # method = "lm" tells ggplot2 to fit the line using linear regression
  geom_smooth(method = "lm") +
  theme_dataedu() +
  labs(x = "Time Spent",
       y = "Final Grade")
```

Looking at this plot, it appears that the more time students spent on the
course, the higher that there final grade is. 

What is the line doing in the upper right part of the graph? Based upon the trend
that is observable in the data, the line of best fit predicts that students who spend 
a particular amount of time on the course *earn greater than 100* for their final grade!
Of course, this is not possible, and highlights the importance of understanding your 
data and carefully interpreting lines of best fit (and other, more sophisticated analyses)
carefully, keeping that understanding and knowledge in mind as you present and make sense 
of the results.

### Linear Model (Regression)

We can find out exactly what the relationship between these two variables is
using a linear model. We discuss linear models in more detail in
[Chapter 10](#c10).

Let's use this technique to model the relationship between the time spent on the
course and students' final grade. Here, we predict
`final_grade`. Students' final, reported grade is the dependent, or *y*-variable, and so
we enter it first, after the `lm()` command and before the tilde (`~`) symbol.
To the right of the tilde is one independent variable, `TimeSpent`, or the time
that students spent on the course. We also pass, or provide, the data frame,
`dat`. At this point, we're ready to run the model. Let's run this line of code
and save the results to an object - we chose `m_linear`, but any name will work.
We will then run the `summary()` function on the output.

``` {r}
m_linear <-
  lm(final_grade ~ TimeSpent, data = dat)

summary(m_linear)
```

Another way that we can generate table output is with a function from the
{sjPlot} package, `tab_model()`. When you run this code, you should see the
results pop up in the "Viewer" pane of RStudio. If you haven't changed the
default settings, this will be in the lower right quadrant of your screen.

``` {r}
tab_model(m_linear,
          title = "Table 7.1")
```

``` {r, echo = F, results = "hide"}
tab_model(m_linear,
          file = "./man/tables/Table 7.1.doc",
          title = "Table 7.1")
```

This will work well for R Markdown documents (or simply to interpret the model
in R). If you want to save the model for use in a Word document, the
[{apaTables}](https://cran.r-project.org/web/packages/apaTables/vignettes/apaTables.html)
(https[]()://cran.r-project.org/web/packages/apaTables/vignettes/apaTables.html)
package may be helpful. To save a table in Word format, just pass the name of
the regression model to a function from the `apaTables` package, like we did
with the `tab_model()` function. Then, you can save the output to a Word
document, simply by adding a `filename` argument:

```{r, eval = FALSE}
apa.reg.table(m_linear, filename = "regression-table-output.doc")
```

You might be wondering what else the {apaTables} package does. We encourage you
to read more about the package here:
<https://cran.r-project.org/web/packages/apaTables/index.html>. The vignette is
especially helpful. One function that may be useful for writing manuscripts is
the following function for creating correlation tables. This function takes, as
an input, a data frame with the variables for which you wish to calculate
correlations.

Before we proceed to the next code chunk, let's talk about some functions we'll
be using a lot in this book. `filter()`, `group_by()`, and `summarize()` are
functions in the {dplyr} package that you will see a lot in upcoming chapters.
You got a preview of these functions earlier in this chapter, and now that
you've seen how they are used, we want to provide clear definitions for each of
these functions.

  - `filter()` removes rows from the dataset that don't match a criteria. Use it
    for tasks like only keeping records for students in the fifth grade
  - `group_by()` groups records together so you can perform operations on those
    groups instead of on the entire dataset. Use it for tasks like getting the
    mean test score of each school instead of a whole school district
  - `summarize()` and `summarise()` reduce your dataset down to a summary
    statistic. Use it for tasks like turning a dataset of student test scores
    into a dataset of grade levels and their mean test score

Now let's use these {dplyr} functions on our survey analysis. We will create the
same measures (based on the survey items) that we used earlier to understand how
they relate to one another.

``` {r}
survey_responses <-
  pre_survey %>%
  # Gather questions and responses
  pivot_longer(cols = q1:q10,
               names_to = "question",
               values_to = "response") %>%
  mutate(
    # Here's where we make the column of question categories
    measure = case_when(
      question %in% c("q1", "q4", "q5", "q8", "q10") ~ "int",
      question %in% c("q2", "q6", "q9") ~ "uv",
      question %in% c("q3", "q7") ~ "pc",
      TRUE ~ NA_character_
    )
  ) %>%
  group_by(student_id, measure) %>%
  # Here's where we compute the mean of the responses
  summarize(
    # Mean response for each measure
    mean_response = mean(response, na.rm = TRUE)
    ) %>%
    # Filter NA (missing) responses
  filter(!is.na(mean_response)) %>%
  pivot_wider(names_from = measure, 
              values_from = mean_response)

survey_responses
```

Now that we've prepared the survey responses, we can use the `apa.cor.table()` function:

```{r}
survey_responses %>% 
  apa.cor.table()
```

The time spent variable is on a very large scale (minutes); what if we
transformed it to represent the number of *hours* that students spent on the
course? Let's use the `mutate()` function we used earlier. We'll end the
variable name in `_hours`, to represent what this variable means.

``` {r}
# creating a new variable for the amount of time spent in hours
dat <- 
  dat %>% 
  mutate(TimeSpent_hours = TimeSpent / 60)

# the same linear model as above, but with the TimeSpent variable in hours
m_linear_1 <- 
  lm(final_grade ~ TimeSpent_hours, data = dat)

# viewing the output of the linear model
tab_model(m_linear_1,
          title = "Table 7.2")
```

``` {r, echo = F, results = "hide"}
tab_model(m_linear_1,
          file = "./man/tables/Table 7.2.doc",
          title = "Table 7.2")
```

The scale still does not seem quite right. What if we standardized the variable
to have a mean of zero and a standard deviation of one?

``` {r}
# this is to standardize the TimeSpent variable to have a mean of 0 and a standard deviation of 1
dat <- 
  dat %>% 
  mutate(TimeSpent_std = scale(TimeSpent))

# the same linear model as above, but with the TimeSpent variable standardized
m_linear_2 <- 
  lm(final_grade ~ TimeSpent_std, data = dat)

# viewing the output of the linear model
tab_model(m_linear_2,
          title = "Table 7.3")
```

``` {r, echo = F, results = "hide"}
tab_model(m_linear_2,
          file = "./man/tables/Table 7.3.doc",
          title = "Table 7.3")
```

When we look at this output, it seems to make more sense. However, there is a
different interpretation now for the time spent variable: for every one standard
deviation increase in the amount of time spent on the course, students' final
grades increases by 8.24, or around eight percentage points.

## Results

Let's extend our regression model and consider the following to be the final
model in this sequence of models: What other variables may matter? Perhaps there
are differences based on the subject of the course. We can add subject as a
variable easily, as follows:

``` {r}
# a linear model with the subject added 
# independent variables, such as TimeSpent_std and subject, can simply be separated with a plus symbol:
m_linear_3 <- 
  lm(final_grade ~ TimeSpent_std + subject, data = dat)
```

We can use `tab_model()` once again to view the results:

``` {r}
tab_model(m_linear_3,
          title = "Table 7.4")
```

``` {r, echo = F, results = "hide"}
tab_model(m_linear_3,
          file = "./man/tables/Table 7.4.doc",
          title = "Table 7.4")
```

It looks like subject `FrSc` - forensic science - and subject `Ocn` -
oceanography - are associated with a higher final grade. This indicates that students in those two classes earned higher grades
than students in other science classes in this dataset.

## Conclusion

In this walkthrough, we focused on taking unprocessed, or raw data, and loading,
viewing, and then processing it through a series of steps. The result was a data
set which we could use to create visualizations and a simple (but powerful!)
linear model, also known as a regression model. We found that the time that
students spent on the course was positively (and statistically significantly)
related to students' final grades, and that there appeared to be differences by
subject. While we focused on using this model in a traditional, explanatory
sense, it could also potentially be used for predictive analytics, in that
knowing how long students spent on the course and what subject their course is
could be used to estimate what that students' final grade might be. We focus on
uses of predictive models further in [Chapter 14](#c14).

In the follow-up to this walkthrough (see [Chapter 13](#c13)), we will focus on
visualizing and then modeling the data using an advanced methodological
technique, multi-level models, using the data we prepared as a part of this data
processing pipeline used in this chapter.

``` {r, eval = FALSE}
write_csv(dat, "data/online-science-motivation/processed/sci_mo_processed.csv")
```