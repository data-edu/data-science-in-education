# Walkthrough 8: Predicting students' final grades using supervised machine learning methods {#c14}

**Abstract** 

This chapter introduces a different kind of analysis that is becoming increasingly common -- supervised machine learning. Such analytic methods rely on identifying an outcome (another name for a dependent variable), and then building (or training) a model to predict that outcome. Some supervised machine learning models are highly complex, whereas others are simple. To illustrate and gain experience training and interpreting a supervised machine learning model, this chapter involves predicting whether students will pasa a class using a deliberately simple model -- a generalized linear model. The Open University Learning Analytics Dataset (OULAD) is used. The tidymodels collection fo packages is used to carry out each of the principal supervised machine learning steps. At the conclusion, ways to build more complex models are discussed.

## Topics emphasized

- Processing data
- Modeling data

## Functions introduced

- initial_split()
- training()
- testing()
- recipe()
- logistic_reg()
- set_model()
- set_mode()
- workflow()
- collect_predictions()
- collect_metrics()

## Vocabulary

- supervised machine learning
- training data
- testing data
- logistic regression
- classification
- predictions
- metrics

## Chapter overview

### Background

In a face-to-face classroom, an educator might count on behavioral cues to help them effectively deliver instruction. Online, educators do not readily have access to the behavioral cues essential for effective face-to-face instruction. For example, in a face-to-face classroom, cues such as a student missing class repeatedly or many students seeming distracted during a lecture can trigger a shift in the delivery of instruction. Many educators find themselves looking for ways to understand and support students online in the same way that face-to-face instructors would. Educational technology affords unique opportunities to support student success online because it provides new methods of collecting and storing data. 

Online learning management systems and Massive Open Online Courses (MOOCs) often automatically track several types of student interactions with the system and feed that data back to the course instructor. For example, an instructor might be able to quickly see how many students logged into their course on a certain day, or they might see how long students engaged with a posted video before pausing it or logging out. The collection of this data is met with mixed reactions from educators. Some are concerned that data collection in this manner is intrusive, but others see a new opportunity to support students in online contexts in new ways. As long as data are collected and utilized responsibly, data collection can support student success.

In this walkthrough, we examine the question, *How well can we predict students who are at risk of dropping a course?*

We focus on *predicting* an outcome--whether a student passes a course--more than *explaining* how variables relate to an outcome, such as how the amount of time students spend on the course relates to their final grade. We do so with a relatively simple machine learning algorithm, or model, a generalized linear regression model. 

### Data sources

We'll be using a widely-used data set in the learning analytics field: the [Open University Learning Analytics Dataset (OULAD)](https://analyse.kmi.open.ac.uk/open_dataset). The OULAD was created by learning analytics researchers at the United Kingdom-based Open University [@kuzilek2017]. It includes data from post-secondary learners participation in one of several Massive Open Online Courses (called *modules* in the OULAD).

The Open University Learning Analytics Dataset (OULAD) provides information for analyzing student outcomes in online learning environments. Many students successfully complete these courses, but not all do, highlighting the importance of identifying those who may be at risk. The dataset includes information on students' initial characteristics and their interactions within the course.

In this chapter, we draw on a widely-used data set in the learning analytics field: the Open University Learning Analytics Dataset (OULAD). The OULAD was created by learning analytics researchers at the United Kingdom-based Open University. It includes data from post-secondary learners participation in one of several Massive Open Online Courses (called modules in the OULAD). 

### Methods

#### Predictive snalytics and machine learning

A buzzword in education software spheres these days is "predictive analytics". Administrators and educators alike are interested in applying the methods long utilized by marketers and other business professionals to try to determine what a person will want, need, or do next. "Predictive analytics" is a blanket term that can be used to describe any statistical approach that yields a prediction. We could ask a predictive model: "What is the likelihood that my cat will sit on my keyboard today?" and, given enough past information about your cat's computer-sitting behavior, the model could give you a probability of that computer-sitting happening today. Under the hood, some predictive models are not very complex. If we have an outcome with two possibilities, a logistic regression model could be fit to the data in order to help us answer the cat-keyboard question. In this chapter, we'll compare a machine learning model to another type of regression: multiple regression. We want to make sure to fit the simplest model as possible to our data. After all, the effectiveness in predicting the outcome is really the most important thing, not the fanciness of the model.
    
Data collection is an essential first step in any type of machine learning or predictive analytics. It is important to note here that machine learning only works effectively when (1) a person selects variables to include in the model that are anticipated to be related to the outcome and (2) a person correctly interprets the model's findings. 

There is an adage: "garbage in, garbage out". This holds true here. If we do not feel confident that the data we collected are accurate, we will not be able to be confident in our conclusions no matter what model we build. To collect good data, we must first clarify what it is that we want to know (i.e., what question are we really asking?) and what information we would need in order to effectively answer that question. Sometimes, people approach analysis from the opposite direction---they might look at the data they have and ask what questions could be answered based on that data. That approach is okay, as long as you are willing to acknowledge that sometimes the pre-existing dataset may *not* contain all the information you need, and you might need to go out and find additional information to add to your dataset to truly answer your question.
    
When people talk about "machine learning", you might get the image in your head of a desktop computer learning how to spell. You might picture your favorite social media site showing you advertisements that are just a little too accurate. At its core, machine learning is the process of "showing" your statistical model only some of the data at once and training the model to predict accurately on that training dataset (this is the "learning" part of machine learning). Then, the model as developed on the training data is shown new data---data you had all along, but hid from your computer initially---and you see how well the model that you developed on the training data performs on this new testing data. Eventually, you might use the model on entirely new data.  

## Load packages

As always, if you have not installed any of these packages before, do so first using the `install.packages()` function. For a description of packages and their installation, review the [Packages](#c06p) section of the [Foundational Skills](#c06) chapter.

```{r, message = FALSE, warning = FALSE}
# load the packages
library(tidyverse)
library(tidymodels)
```


#### Reading CSV Data

To begin, we will load our data using the `read_csv()` function. This dataset has undergone minimal preprocessing to help streamline our analysis. Additionally, we will load an assessments dataset, which provides data on students' performance on various assessments throughout their courses.

```{r}
students <- read_csv("data/ml/oulad-students.csv")
assessments <- read_csv("data/ml/oulad-assessments.csv")
```

The dataset integrates information from three sources: `studentInfo`, `courses`, and `studentRegistration`. For a more comprehensive understanding of the variables, you can review the dataset description [here](https://analyse.kmi.open.ac.uk/open_dataset#description).

### Inspecting the Data

Let’s take a quick look at the structure of both datasets using the `glimpse()` function. This provides an overview of the data types and the first few values of each variable.

```{r}
glimpse(students)
glimpse(assessments)
```

### Creating New Variables

Our primary goal is to build a model that can predict whether a student is at risk of dropping out of a course. To achieve this, we need to create an outcome variable indicating whether a student has passed.

Using the `mutate()` function, we can create a binary variable that represents whether the student passed (1) or did not pass (0):

```{r}
students <- students %>%
    mutate(pass = ifelse(final_result == "Pass", 1, 0)) %>%
    mutate(pass = as.factor(pass))
```

The [`mutate()`](https://dplyr.tidyverse.org/reference/mutate.html#:~:text=mutate()%20creates%20new%20columns,setting%20their%20value%20to%20NULL%20) function is essential for creating new variables, modifying existing ones, or even removing columns by setting their values to `NULL`.

Next, we will create a factor variable to indicate whether a student identifies as having a disability. This variable will serve as one of our predictors.

```{r}
students <- students %>% 
    mutate(disability = as.factor(disability))
```

We will also summarize assessment data to create a new predictor based on students’ performance on assessments submitted early in the course. Specifically, we will calculate the mean weighted score of assessments submitted before the first quartile of assignment dates.

```{r}
code_module_dates <- assessments %>% 
    group_by(code_module, code_presentation) %>% 
    summarize(quantile_cutoff_date = quantile(date, probs = .25, na.rm = TRUE))

assessments_joined <- assessments %>% 
    left_join(code_module_dates) %>% 
    filter(date < quantile_cutoff_date) %>% 
    mutate(weighted_score = score * weight) %>% 
    group_by(id_student) %>% 
    summarize(mean_weighted_score = mean(weighted_score, na.rm = TRUE))

students <- students %>% 
    left_join(assessments_joined, by = "id_student")
```

To verify that these new variables have been correctly added, you can manually inspect the data using the `view()` function:

```{r, eval=FALSE}
view(students)
```

### Exploratory Data Analysis

Exploratory data analysis (EDA) helps us better understand the dataset and identify potential issues or patterns. In this section, we will:

1. Count the number of students and the number of course offerings.
2. Engineer a new variable representing socioeconomic status.

#### Examining Outcomes

We start by counting the total number of students and the specific course offerings. The `count()` function is useful for this purpose.

```{r}
students %>% 
    count(id_student) # counts total students

students %>% 
    count(code_module, code_presentation) # counts course offerings

assessments %>% 
    count(assessment_type) # counts different types of assessments
```

#### Feature Engineering: Socioeconomic Index

Feature engineering involves creating new variables that can enhance the predictive power of our model. Here, we will create a new predictor variable representing socioeconomic status based on the index of multiple deprivation (IMD) variable.

```{r}
students <- students %>% 
    mutate(imd_band = factor(imd_band, levels = c("0-10%",
                                        "10-20%",
                                        "20-30%",
                                        "30-40%",
                                        "40-50%",
                                        "50-60%",
                                        "60-70%",
                                        "70-80%",
                                        "80-90%",
                                        "90-100%"))) %>%
    mutate(imd_band = as.integer(imd_band))

students
```

### Building the Predictive Model

With our dataset prepared, we can now proceed to build a predictive model. This involves five key steps:

1. Splitting the data into training and test sets.
2. Creating a preprocessing recipe.
3. Specifying the model and workflow.
4. Fitting the model to the training data.
5. Evaluating the model’s accuracy.

#### Step 1: Splitting the Data

Machine learning models are typically trained on a subset of the data and tested on a separate subset to evaluate performance. We will use an 80-20 split, with 80% of the data used for training and 20% reserved for testing.

```{r}
set.seed(20230712)

students <- students %>% 
    drop_na(mean_weighted_score)

train_test_split <- initial_split(students, prop = .80, strata = "pass")

data_train <- training(train_test_split)

data_test  <- testing(train_test_split)
```

To confirm the split, you can check the number of observations in `data_train` and `data_test`:

```{r}
data_train
data_test
```

#### Step 2: Creating a Recipe

The {recipes} package allows us to define preprocessing steps for our model. We will create a recipe that predicts the outcome (`pass`) using `disability`, `imd_band`, and `mean_weighted_score` as predictor variables.

```{r}
my_rec <- recipe(pass ~ disability + imd_band + mean_weighted_score, data = data_train)

my_rec
```

#### Step 3: Specifying the Model and Workflow

We specify a logistic regression model using the {parsnip} package. Since our outcome is binary, we set the mode to "classification" and use the "glm" engine (generalized linear model).

```{r}
my_mod <-
    logistic_reg() %>% 
    set_engine("glm") %>% 
    set_mode("classification")

my_mod
```

Next, we bundle the recipe and model into a workflow using the {workflows} package:

```{r}
my_wf <-
    workflow() %>% 
    add_model(my_mod) %>% 
    add_recipe(my_rec)
```

#### Step 4: Fitting the Model

We can now fit our model to the training data and evaluate it using the test data.

```{r}
final_fit <- last_fit(my_wf, train_test_split)
```

#### Step 5: Evaluating Model Accuracy

To assess the model’s accuracy, we compare its predictions to the actual outcomes in the test set.

```{r}
final_fit %>% 
    collect_predictions() %>% 
    select(.pred_class, pass) %>% 
    mutate(correct = .pred_class == pass)
```

For a quick summary of model performance, we can use the `collect_metrics()` function:

```{r}
collect_metrics(final_fit)
```

This chapter introduced the fundamental steps involved in building a predictive model using logistic regression. In the next chapter, we will explore more advanced models and feature engineering techniques to improve predictive performance.

## Conclusion

Though we focus on this relatively simple model, or algorithm,, many of the ideas explored in this chapter will likely extend and prove useful for other machine learning methods. Our goal is for you to finish this final walkthrough with the confidence to explore using machine learning to answer a question or to solve a problem of your own with respect to teaching, learning, and educational systems.

In this chapter, we introduced general machine learning ideas, like training and test datasets and evaluating the importance of specific variables, in the context of predicting students' passing a course. Like many of the topics in this book---but, perhaps *particularly* so for machine learning---there is much more to discover on the topic, and we encourage you to consult the books and resources in the [Learning More chapter](#c17) to learn about further applications of machine learning methods.
