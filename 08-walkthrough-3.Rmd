---
title: 'Education Dataset Analysis Pipeline: Walk Through #3'
output: html_document
---

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, eval = TRUE, echo = FALSE, results = 'hide',
                      message = FALSE, warning = FALSE)
```

# Background

One area of interest is the delivery of online instruction, which is becoming more prevalent: in 2007, over 3.9 million U.S. students were enrolled one or more online courses (Allen & Seaman, 2008). With the dawn of online learning comes an abundance of new educational tools to facilitate that learning. Indeed, online learning interfaces are used to facilitate the submission of assignments and quizzes in courses in which students and instructor meet face-to-face, but these interfaces are also used in fully online courses to deliver all instruction and assessment. 

In a face-to-face classroom, an educator might count on behavioral cues to help them effectively deliver instruction. However, one constraint of online education is that educators do not have access as readily to the behavioral cues that can be essential to effective face-to-face instruction. For example, in a face-to-face classroom, cues such as a student missing class repeatedly or many students seeming distracted during a lecture can trigger a shift in the delivery of instruction. While technology is rapidly developing, many educators find themselves looking for ways to understand and support students online in the same way that face-to-face instructors would. Educational technology affords unique opportunities to support student success online because it provides new methods of collecting and storing data. 

Indeed, online learning management systems often automatically track several types of student interactions with the system and feed that data back to the course instructor. For example, an instructor might be able to quickly see how many students logged into their course on a certain day, or they might see how long students engaged with a posted video before pausing it or logging out. The collection of this data is met with mixed reactions from educators. Some are concerned that data collection in this manner is intrusive, but others see a new opportunity to support students in online contexts in new ways. As long as data are collected and utilized responsibly, data collection can support student success.

One meaningful perspective from which to consider students' engagement with online courses is related to their motivation to achieve. More specifically, it is important to consider how and why students are engaging with the course. Considering the psychological mechanisms behind achievement is valuable because doing so may help to identify meaningful points of intervention for educators and for researchers and administrators in online *and* face-to-face courses interested in the intersection between behavioral trace measures and students' motivational and emotional experiences in such courses.

In this walkthrough, we examine the educational experiences of students in online science courses at a virtual middle school in order to characterize their motivation to achieve and their tangible engagement with the course in terms of behavioral trace measures. To do so, we use a robust data set, which includes self-reported motivation as well as behavioral trace data collected from a learning management system (LMS) to identify predictors of final course grade. Our work examines the idea of educational success in terms of student interactions with an online science course.

We explore the following four questions:

1. Is motivation more predictive of course grades as compared to other online indicators of engagement?
2. Which types of motivation are most predictive of achievement?
3. Which types of trace measures are most predictive of achievement?
4. How does a random forest compare to a simple linear model (regression)?

# Information about the dataset 

This dataset came from 499 students enrolled in online middle school science courses in 2015-2016. The data were originally collected for use as a part of a research study, though the findings have not been published anywhere yet.

The setting of this study was a public provider of individual online courses in a Midwestern state. In particular, the context was two semesters (Fall and Spring) of offerings of five online science courses (Anatomy & Physiology, Forensic Science, Oceanography, Physics, and Biology), with a total of 36 classes. 

Specific information in the dataset included:

- a pre-course survey students completed about their self-reported motivation in science — in particular, their perceived competence, utility value, and interest
- the time students spent on the course (obtained from the learning management system (LMS), Blackboard
- students' final course grades 
- students' involvement in discussion forums

For discussion board responses, we used the Linguistic Inquiry and Word Count (LIWC; Pennebaker, Boyd, Jordan, & Blackburn, 2015) to calculate the number of posts per student and variables for the mean levels of students' cognitive processing, positive emotions, negative emotions, and social-related discourse.
<<<<Josh, is this preceding paragraph jargony>>>>

At the beginning of the semester, students were asked to complete the pre-course survey about their perceived competence, utility value, and interest. At the end of the semester, the time students spent on the course, their final course grades, and the contents of the discussion forums were collected.

In this walkthrough, we used the R package **caret** to carry out the analyses.

# Selecting an analysis

## Defining our research question
When you begin a new project, there are often many approaches to analyzing data and answering questions you might have about it. Some projects have a clearly defined scope and question to answer. This type of project is characterized by 1) a defined number of variables (data inputs) and 2) specific directional hypotheses. For example, if we are studying the effect of drinking coffee after dinner on ability to quickly fall asleep, we might have a very specific directional hypothesis: we expect that drinking coffee after dinner would decrease the ability to fall asleep quickly. In this case, we might collect data by having some people drink coffee and having other people drink nothing or an herbal tea before bed. We could monitor how quickly people from each group fall asleep. Since we collected data from two clearly defined groups, we can then do a statistical analysis that compares the amount of time it takes to fall asleep for each group. One option would be a test called a t-test, which we could use to see if there is a significant difference in the average amount of minutes to fall asleep for the group. This approach works very well in controlled experimental situations, especially when we can change only one thing at a time (in our coffee example, the only thing we changed was the coffee-drinking behavior of our participants - all other life conditions were held equal for both groups). Rarely are educational data projects as clear-cut and simple.

For this walkthrough, we have many sources of data - survey data, learning management system data, discussion forum data, and academic achievement data as measured by final course grades. Luckily, having too much data is what we call a "good problem." In our coffee example above, we had one really specific idea that we wanted to investigate - does coffee affect time taken to fall asleep? In this walkthrough we have many ideas we are curious to explore: the relationships among motivation, engagement in the course (discussion boards, time spent online in the course site), and academic achievement. If we wanted to tackle a simpler problem, we could choose just one of these relationships. For example, we could measure whether students with high motivation earn higher grades than students with low motivation. However, we are being a bit more ambitious than that here - we are interested in understanding the complex relationships among the different types of motivation. Rather than simply exploring whether A affects B, we are interested in the nuances: we suspect that *many* factors affect B, and we would like to see which of those factors has most relative importance. To explore this idea, we will use a machine learning approach.

## Predictive analytics and machine learning
A buzzword in education software spheres these days is "predictive analytics." Administrators and educators alike are interested in applying the methods long utilized by marketers and other business professionals to try to determine what a person will want, need, or do next. "Predictive analytics" is a blanket term that can be used to describe any statistical approach that yields a prediction. We could ask a predictive model: "What is the likelihood that my cat will sit on my keyboard today?" and, given enough past information about your cat's computer-sitting behavior, the model could give you a probability of that computer-sitting happening today. Under the hood, some predictive models are not very complex. If we have an outcome with two possibilities, a logistic regression model could be fit to the data in order to help us answer the cat-keyboard question. In this chapter, we'll compare a machine learning model to another type of regression: multiple regression. We want to make sure to fit the simplest model as possible to our data. After all, the effectiveness in predicting the outcome is really the most important thing: not the fanciness of the model.
    
Data collection is an essential first step in any type of machine learning or predictive analytics. It is important to note here that machine learning only works effectively when (1) a person selects variables to include in the model that are anticipated to be related to the outcome and (2) a person correctly interprets the model's findings. There is an adage that goes, "garbage in, garbage out." This holds true here: if we do not feel confident that the data we collected are accurate, no matter what model we build, we will not be able to be confident in our conclusions. To collect good data, we must first clarify what it is that we want to know (i.e., what question are we really asking?) and what information we would need in order to effectively answer that question. Sometimes, people approach analysis from the opposite direction - they might look at the data they have and ask what questions could be answered based on that data. That approach is okay - as long as you are willing to acknowledge that sometimes the pre-existing dataset may *not* contain all the information you need, and you might need to go out and find additional information to add to your dataset to truly answer your question.
    
When people talk about "machine learning," you might get the image in your head of a desktop computer learning how to spell. You might picture your favorite social media site showing you advertisements that are just a little too accurate. At its core, what machine learning really is is the process of "showing" your statistical model only some of the data at once, and training the model to predict accurately on that training dataset (this is the "learning" part of machine learning). Then, the model as developed on the training data is shown new data - data you had all along, but hid from your computer initially - and you see how well the model that you developed on the training data performs on this new testing data. Eventually, you might use the model on entirely new data.  

# Information on random forests
For our analyses, we used Random Forest modeling (Breiman, 2001). Random forest is an extension of decision tree modeling, whereby a collection of decision trees are simultaneously "grown" and are evaluated based on out-of-sample predictive accuracy (Breiman, 2001).  Random forest is random in two main ways: first, each tree is only allowed to "see" and split on a limited number of predictors instead of all the predictors in the parameter space; second, a random subsample of the data is used to grow each individual tree, such that no individual case is weighted too heavily in the final prediction. 

One thing about random forest that makes it quite different from other types of analysis we might do is that here, we are giving the computer a large amount of information and asking it to find connections that might not be immediately visible to the naked human eye. This is great for a couple of reasons. First, while humans are immensely creative and clever, we are not immune to biases. If we are exploring a dataset, we usually come in with some predetermined notions about what we think is true, and we might (consciously or unconsciously) seek evidence that supports the hypothesis we privately hold. By setting the computer loose on some data, we can learn that there are connections between areas that we did not expect. We must also be ready for our hypotheses to not be supported! Random forest is particularly well-suited to the research questions explored here because we do not have specific directional hypotheses. Machine learning researchers talk about this as "exploring the parameter space" - we want to see what connections exist, and we acknowledge that we might not be able to accurately predict all the possible connections. Indeed, we expect - and hope - that we will find surprising connections. 

Whereas some machine learning approaches (e.g., boosted trees) would utilize an iterative model-building approach, random forest estimates all the decision trees at once. In this way, each tree is independent of every other tree. Thus, the random forest algorithm provides a robust regression approach that is distinct from other modeling approaches. The final random forest model aggregates the findings across all the separate trees in the forest in order to offer a collection of "most important" variables as well as a percent variance explained for the final model.

500 trees were grown as part of our random forest. We partitioned the data before conducting the main analysis so that neither the training nor the testing data set would be disproportionately representative of high-achieving or low-achieving students. The training data set consisted of 80% of the original data (n = 400 cases), whereas the testing data set consisted of 20% of the original data (n = 99 cases). We built our random forest model on the training data set, and then evaluated the model on the testing data set. Three variables were tried at each node.

Note that the random forest algorithm does not accept cases with missing data, and so we deleted cases listwise if data were missing. This decision eliminated 51 cases from our original data set, to bring us to our final sample size of 499 unique students. If you have a very small dataset with a lot of missing data, the random forest approach may not be well suited for your goals – you might consider a linear regression instead. 

A random forest is well suited to the research questions that we had here because it allows for nonlinear modeling. We hypothesized complex relationships between students' motivation, their engagement with the online courses, and their achievement. For this reason, a traditional regressive or structural equation model would have been insufficient to model the parameter space we were interesting in modeling. Our random forest model had one outcome and eleven predictors. 

One term you will hear used in machine learning is "tuning parameter." People often think of tuning parameters as knobs or dials on a radio: they are features of the model that can be adjusted to get the clearest signal. A common tuning parameter for machine learning models is the number of variables considered at each split (Kuhn, 2008); we considered three variables at each split for this analysis.  

The outcome was the final course grade that the student earned. The predictor variables included motivation variables (interest value, utility value, and science perceived competence) and trace variables (the amount of time spent in the course, the course name, the number of discussion board posts over the course of the semester, the mean level of cognitive processing evident in discussion board posts, the positive emotions evident in discussion board posts, the negative emotions evident in discussion board posts, and the social-related discourse evident in their discussion board posts). We used this random forest model to address all three of our research questions.

To interpret our findings, we examined three main factors: (1) predictive accuracy of the random forest model, (2) variable importance, and (3) variance explained by the final random forest model.

# Analysis

```{r}
library(tidyverse)
library(caret)
library(here)
library(tidylog)
```

First, we will load the data, *filter* the data to include only the data from one year, and *select* variables of interest.

```{r}
#loading the data as a .csv file
f <- here::here("online-science-motivation-w-disc.csv")

d <- read_csv(f)

#filtering the data to only include spring 2017 data
d <-d %>% filter(!str_detect(course_ID, "S217"))

#selecting only the variables we are interested in: motivation, time spent in the course, grade in the course, subject, 
    #enrollment information, positive and negative emotions, and cognitive processing
d <- d %>% 
    select(pre_int, pre_uv,  pre_percomp, time_spent,course_ID, final_grade, subject, enrollment_reason, semester, enrollment_status, cogproc, social, posemo, negemo, n)
```

## Use of caret

Here, we remove observations with missing data (per our note above about random forests requiring complete cases).

```{r}
#checking how many rows are in our dataset
nrow(d)
    #we see that we have 550

#calling the na.omit function to eliminate ANY rows that have ANY missing data
d <- na.omit(d)

#checking whether our na.omit call worked as expected
nrow(d)
    #after running the code above, we see that we now have 499 rows - this is as we expected
```

First, machine learning methods often involve using a large number of variables. Oftentimes, some of these variables will not be suitable to use: they may be highly correlated with other variables, for instance, or may have very little - or no - variability. Indeed, for the data set used in this study, one variable has the same (character string) value for all of the observations. We can detect this variable and any others using the following function:

```{r}
#we run the nearZeroVar function to determine if there are variables with NO variability
nearZeroVar(d, saveMetrics = TRUE)
```

If we look at `enrollment_status`, we will see that it is "Approved/Enrolled" for *all* of the students. In the nearZeroVar function we just ran, we see a result in the ZeroVar column of "TRUE" for the `enrollment_status` variable. When we use variables with no variability in certian models, it may cause some problems, and so we remove it first.

```{r}
#Taking the dataset and re-saving it as the same dataset, but without the enrollment status variable
d <- select(d, -enrollment_status)
```

Note that many times you may wish to pre-process the variables, such as by centering or scaling them. Often the data will come to you in a format that is not ready for immediate analysis, as we have discussed elsewhere in the book. For our current dataset, we could work on pre-processing with code like you will see below. We set this next code chunk up to not run here (if you are viewing the book online), as we will first try this out with the variables' original values.

```{r, eval = FALSE}
#example pre-processing step: manipulating the dataset 'd' so that if a variable is numeric, its format will now be scale
d <- mutate_if(d, 
               is.numeric, scale)
```

As another pre-processing step, we want to make sure our text data is in a format that we can easily evaluate. To facilitate that, we want to make character string variables into factors.

```{r}
#converting the text (character) variables in our dataset into factors
d <- mutate_if(d, is.character, as.factor)
```

Now, we will prepare the **train** and **test** datasets, using the caret function for creating data partitions. Here, the **p** argument specifies what proportion of the data we want to be in the **training** partition. Note that this function splits the data based upon the outcome, so that the training and test data sets will both have comparable values for the outcome. This means that since our outcome is final grade, we are making sure that we don't have either a training or testing dataset that has too many good grades - or too many bad grades. Note the `times = 1` argument; this function can be used to create *multiple* train and test sets, something we will describe in more detail later. Before we create our training and testing datasets, we want to initiate a process called "setting the seed." This means that we are ensuring that if we run this same code again, we will get the same results in terms of the data partition. The seed can be any number that you like - some people choose their birthday or another meaningful number. The only constraint is that wehn you open the same code file again to run in the future, you do not change the number you selected for your seed. This ensures your code is reproducible. In fact, it ensures that anyone who runs the same code file on any computer, anywhere, will get the same result. With that background information, try running the code chunk below.

```{r}
#First, we set a seed to ensure the reproducibility of our data partition.
set.seed(62020)

#we create a new object called trainIndex that will take 80 percent of the data
trainIndex <- createDataPartition(d$final_grade,
                                  p = .8, 
                                  list = FALSE,
                                  times = 1)


#We add a new variable to our dataset, temporarily:
    #this will let us select our rows according to their row number
    #we populate the rows with the numbers 1:499, in order

d <- d %>% 
    mutate(temp_id = 1:499)

#we filter our dataset so that we get only the 
    #rows indicated by our "trainIndex" vector
d_train <- filter(d, temp_id %in% trainIndex)

#we filter our dataset in a different way so that we get only  the rows NOT in our "trainIndex" vector
    #adding the ! before the temp_id variable achieves the opposite of what we did in the line of code above

d_test <- filter(d, !temp_id %in% trainIndex)

#We delete the temp_id variable from (1) the original data, (2) the portion of the original data we marked as training, and (3) the portion of the original data we marked as testing, as we no longer need that variable

d <- select(d, -temp_id)
d_train <- select(d_train, - temp_id)
d_test <- select(d_test, -temp_id)
```

Finally, we will estimate the models.

Here, we will use the train function, passing *all* of the variables in the data frame (except for the outcome, or dependent variable, `final_grade`) as predictors. Note that you can read more about the specific random forest implementation chosen [here](http://topepo.github.io/caret/train-models-by-tag.html#random-forest). To specify that we want to predict the outcome using every variable except the outcome itself, we use the formulation (outcome ~ .,). R interprets this code as: predict the outcome using all the variables except outcome itself. The outcome always comes before the `~`, and the `.` that we see after the `~` means that we want to use all the rest of the variables. An alternative specification of this model would be to write (outcome ~ predictor1, predictor2). Anything that follows the `~` and precedes the comma is treated as predictors of the outcome.


Here, we set the seed again, to ensure that our analysis is reproducible. This step of setting the seed is especially important due to the "random" elements of random forest, because it's likely that the findings would change (just slightly) if the seed were not set.

```{r}
#setting a seed for reproducibility
set.seed(62020)

#we run the model here
rf_fit <- train(final_grade ~ .,
                data = d_train,
                method = "ranger")

#here, we get a summary of the model we just built
rf_fit
```


We have some results! First, we see that we have 400 samples, or 400 observations, the number in the train data set. No pre-processing steps were specified in the model fitting, but note that the output of `preProcess` can be passed to `train()` to center, scale, and transform the data in many other ways. Next, in our example, a resampling technique has been used. This resampling is not for validating the model (per se), but is rather for selecting the tuning parameters - the options that need to be specified as a part of the modeling. These parameters can be manually provided, or can be estimated via strategies such as the bootstrap resample (or *k*-folds cross validation).

As we interpret these findings, we are looking to minimize the error (RMSE) and maximize the variance explained (rsquared).

It appears that the model with the value of the **mtry** tuning parameter equal to 43 seemed to explain the data best, the **splitrule** being "extratrees", and **min.node.size** held constant at a value of 5. 

Let's see if we end up with slightly different values if we change the resampling technique to cross-validation, instead of bootstrap resampling. We set a seed again here, for reproducibility.

```{r}
set.seed(62020)

train_control <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 10)

rf_fit1 <- train(final_grade ~ .,
                data = d_train,
                method = "ranger",
                trControl = train_control)

rf_fit1
```

The same tuning parameter value seems to be found with this method: mtry 43. Let's check just one last thing - what if we do not fix **min.node.size** to five?

Let's create our own grid of values to test. We'll stick with the default bootstrap resampling method to choose the best model.

```{r}
#create a grid of different values of mtry, different splitrules, and different minimum node sizes to test
tune_grid <- expand.grid(mtry = c(2, 22, 43),
                         splitrule = c("variance", "extratrees"),
                         min.node.size = c(1, 5, 10, 15, 20))

#set a seed
set.seed(62020)

#fit a new model, using the tuning grid we created above
rf_fit2 <- train(final_grade ~ .,
                data = d_train,
                method = "ranger",
                tuneGrid = tune_grid)

rf_fit2
```

The model with the same values as identified before for **mtry** and **splitrule**, but with **min.node.size** equal to 1 seems to fit best, though the improvement seems to be fairly small relative to the difference the other tuning parameters seem to make. 

Let's take a look at this model. The independent variables are identified as "predictor" variables in the model summary, right below the line indicating the number of samples. We will first note the large number of independent variables, 13 in this case: this is due to the factors being treated as dummy codes.We can see values for the RMSE (13.20),R-squared (0.58), and MAE (9.77). While the output above gives us a good summary of the model, we might want to look more closely at what we found with our rf_fit2 model. The code below is a way for us to zoom in and look specifically at the *final* random forest model generated by our rf_fit2.


In the code chunk below, you'll notice we are selecting the "finalModel" output using a `$` operator rather than the familiar `select`. We cannot use dplyr and the tidyverse here because of the structure of the rf_fit2 object - it's not a normal dataframe. Thus, we extract with a `$`.

```{r}
#Here, we select the "finalModel" output from the rf_fit2 model
rf_fit2$finalModel
```
When we run the above code, we can also note the *OOB prediction error (MSE)*, of 167.79, and the proportion of the variance explained, or R squared, of 0.597.

Now that we understand how to develop a machine learning model, and how to use different tuning parameters (such as node size and the splitting rule), we can explore some other related themes. We might wonder about how we could examine the predictive accuracy of the random forest model we just developed.

# Examining predictive accuracy on the test data set

What if we use the test data set - data not used to train the model?

```{r}
set.seed(62020)
d_test_augmented <- mutate(d_test,
                           pred = predict(rf_fit2, d_test),
                           obs = final_grade)

defaultSummary(as.data.frame(d_test_augmented))
```

We can compare this to the values above to see how much poorer - if at all - performance is on data not used to train the model. Comparing the RMSE values, we see that the RMSE is a bit lower, but not much different, when we use the test set to train the model. We see the RMSE is 11.58 here, whereas it was 13.20 above. The Rsquared value is 0.74 here, as compared to 0.58 for the original model for rf_fit2. Last, the MAE is 8.68  when trained on the test data, as compared to 0.977 on the train data.

## Variable importance measures

One helpful characteristic of random forest models is that we can learn about which variables contributed most strongly to the predictions in our model, across all the trees in our forest.

We can examine two different variable importance measures using the **ranger** method in **caret**.

Note that importance values are not calculated automatically, but that "impurity" or "permutation" can be passed to the `importance` argument in `train()`. See more [here](https://alexisperrier.com/datascience/2015/08/27/feature-importance-random-forests-gini-accuracy.html).

We'll re-run the model, but will add an argument to call the variable importance metric.

```{r}
rf_fit2_imp <- train(final_grade ~ .,
                data = d_train,
                method = "ranger",
                tuneGrid = tune_grid,
                importance = "impurity")

varImp(rf_fit2_imp)
```

We can visualize these:

```{r}
varImp(rf_fit2_imp) %>%
    pluck(1) %>%
    rownames_to_column("var") %>%
    ggplot(aes(x = reorder(var, Overall), y = Overall)) +
    geom_col() +
    coord_flip()
```

We can see whether these change with the different importance measures.

```{r}
rf_fit2_imp_permutation <- train(final_grade ~ .,
                data = d_train,
                method = "ranger",
                importance = "permutation")

varImp(rf_fit2_imp_permutation) %>%
    pluck(1) %>%
    rownames_to_column("var") %>%
    ggplot(aes(x = reorder(var, Overall), y = Overall)) +
    geom_col() +
    coord_flip()
```

They are similar but somewhat different. One variable that shows up as important in both of the variable importance visualizations is `subjectFrScA`, a variable which means ___________________________.
One takeaway from this analysis is that what course students are in seems to have a different effect depending on the course. Also, how much students write in their discussion posts (`n`) seems to be very important - as does the time students spend in their course. Finally, there are some subject level differences (in terms of how predictive subject is). Perhaps grades should be normalized within subject: would this still be an important predictor if we did that? We won't dive into that question here, but you can see how the line of research inquiry might progress as you start to explore your data with a machine learning model.

## Comparing a random forest to a regression

You may be curious about comparing the predictive accuracy of the model to a linear model (a regression).

```{r, warning = FALSE}
#Make sure all variables stored as characters are converted to factors
d_train_lm <- mutate_if(d_train, is.character, as.factor) 

#Create a linear regression model, using the same formula approach as in the random forest: ~ .
lm_fit <- train(final_grade ~ .,
                data = d_train_lm, 
                method = "lm")

#Append the predicted values to the training dataset for the linear model, so we can see both the predicted and the actual values
d_train_lm <- mutate(d_train, 
                     obs = final_grade,
                     pred = predict(lm_fit, d_train_lm))

#Append the predicted values to the training dataset for the random forest
d_train_randomfor <- mutate(d_train, 
                  pred = predict(rf_fit2, d_train),
                  obs = final_grade)

#Summarize, as data frames, the training data with the predicted and the actual values
    #for both the linear model
defaultSummary(as.data.frame(d_train_lm))
    #and the random forest
defaultSummary(as.data.frame(d_train_randomfor))
```

We can see that the random forest technique seems to perform better than regression. Specifically, the RMSE is lower for the random forest (4.81 as compared to 13.13 for the linear model). Second, the Variance explained is much higher in the random forest (0.96 as compared to 0.59 for the linear model). It may be interesting to compare the results from the random forest not to a more straightforward model, such as a regression, but to a more sophisticated model, like one for deep learning. For now, we'll leave that to you.

In this walkthrough, we used the R package **caret** to carry out the analyses.
