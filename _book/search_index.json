[["c14.html", "14 Walkthrough 8: Predicting students’ final grades using supervised machine learning methods 14.1 Topics emphasized 14.2 Functions introduced 14.3 Vocabulary 14.4 Chapter overview 14.5 Load packages 14.6 Conclusion", " 14 Walkthrough 8: Predicting students’ final grades using supervised machine learning methods Abstract This chapter introduces a different kind of analysis that is becoming increasingly common – supervised machine learning. Such analytic methods rely on identifying an outcome (another name for a dependent variable), and then building (or training) a model to predict that outcome. Some supervised machine learning models are highly complex, whereas others are simple. To illustrate and gain experience training and interpreting a supervised machine learning model, this chapter involves predicting whether students will pasa a class using a deliberately simple model – a generalized linear model. The Open University Learning Analytics Dataset (OULAD) is used. The tidymodels collection fo packages is used to carry out each of the principal supervised machine learning steps. At the conclusion, ways to build more complex models are discussed. 14.1 Topics emphasized Processing data Modeling data 14.2 Functions introduced initial_split() training() testing() recipe() logistic_reg() set_model() set_mode() workflow() collect_predictions() collect_metrics() 14.3 Vocabulary supervised machine learning training data testing data logistic regression classification predictions metrics 14.4 Chapter overview 14.4.1 Background In a face-to-face classroom, an educator might count on behavioral cues to help them effectively deliver instruction. Online, educators do not readily have access to the behavioral cues essential for effective face-to-face instruction. For example, in a face-to-face classroom, cues such as a student missing class repeatedly or many students seeming distracted during a lecture can trigger a shift in the delivery of instruction. Many educators find themselves looking for ways to understand and support students online in the same way that face-to-face instructors would. Educational technology affords unique opportunities to support student success online because it provides new methods of collecting and storing data. Online learning management systems and Massive Open Online Courses (MOOCs) often automatically track several types of student interactions with the system and feed that data back to the course instructor. For example, an instructor might be able to quickly see how many students logged into their course on a certain day, or they might see how long students engaged with a posted video before pausing it or logging out. The collection of this data is met with mixed reactions from educators. Some are concerned that data collection in this manner is intrusive, but others see a new opportunity to support students in online contexts in new ways. As long as data are collected and utilized responsibly, data collection can support student success. In this walkthrough, we examine the question, How well can we predict students who are at risk of dropping a course? We focus on predicting an outcome–whether a student passes a course–more than explaining how variables relate to an outcome, such as how the amount of time students spend on the course relates to their final grade. We do so with a relatively simple machine learning algorithm, or model, a generalized linear regression model. 14.4.2 Data sources We’ll be using a widely-used data set in the learning analytics field: the Open University Learning Analytics Dataset (OULAD). The OULAD was created by learning analytics researchers at the United Kingdom-based Open University (Kuzilek et al., 2017). It includes data from post-secondary learners participation in one of several Massive Open Online Courses (called modules in the OULAD). The Open University Learning Analytics Dataset (OULAD) provides information for analyzing student outcomes in online learning environments. Many students successfully complete these courses, but not all do, highlighting the importance of identifying those who may be at risk. The dataset includes information on students’ initial characteristics and their interactions within the course. In this chapter, we draw on a widely-used data set in the learning analytics field: the Open University Learning Analytics Dataset (OULAD). The OULAD was created by learning analytics researchers at the United Kingdom-based Open University. It includes data from post-secondary learners participation in one of several Massive Open Online Courses (called modules in the OULAD). 14.4.3 Methods 14.4.3.1 Predictive snalytics and machine learning A buzzword in education software spheres these days is “predictive analytics”. Administrators and educators alike are interested in applying the methods long utilized by marketers and other business professionals to try to determine what a person will want, need, or do next. “Predictive analytics” is a blanket term that can be used to describe any statistical approach that yields a prediction. We could ask a predictive model: “What is the likelihood that my cat will sit on my keyboard today?” and, given enough past information about your cat’s computer-sitting behavior, the model could give you a probability of that computer-sitting happening today. Under the hood, some predictive models are not very complex. If we have an outcome with two possibilities, a logistic regression model could be fit to the data in order to help us answer the cat-keyboard question. In this chapter, we’ll compare a machine learning model to another type of regression: multiple regression. We want to make sure to fit the simplest model as possible to our data. After all, the effectiveness in predicting the outcome is really the most important thing, not the fanciness of the model. Data collection is an essential first step in any type of machine learning or predictive analytics. It is important to note here that machine learning only works effectively when (1) a person selects variables to include in the model that are anticipated to be related to the outcome and (2) a person correctly interprets the model’s findings. There is an adage: “garbage in, garbage out”. This holds true here. If we do not feel confident that the data we collected are accurate, we will not be able to be confident in our conclusions no matter what model we build. To collect good data, we must first clarify what it is that we want to know (i.e., what question are we really asking?) and what information we would need in order to effectively answer that question. Sometimes, people approach analysis from the opposite direction—they might look at the data they have and ask what questions could be answered based on that data. That approach is okay, as long as you are willing to acknowledge that sometimes the pre-existing dataset may not contain all the information you need, and you might need to go out and find additional information to add to your dataset to truly answer your question. When people talk about “machine learning”, you might get the image in your head of a desktop computer learning how to spell. You might picture your favorite social media site showing you advertisements that are just a little too accurate. At its core, machine learning is the process of “showing” your statistical model only some of the data at once and training the model to predict accurately on that training dataset (this is the “learning” part of machine learning). Then, the model as developed on the training data is shown new data—data you had all along, but hid from your computer initially—and you see how well the model that you developed on the training data performs on this new testing data. Eventually, you might use the model on entirely new data. 14.5 Load packages As always, if you have not installed any of these packages before, do so first using the install.packages() function. For a description of packages and their installation, review the Packages section of the Foundational Skills chapter. # load the packages library(tidyverse) library(tidymodels) 14.5.0.1 Reading CSV Data To begin, we will load student-level data using the read_csv() function. This dataset has undergone minimal preprocessing to help streamline our analysis – it integrates information from three sources: studentInfo, courses, and studentRegistration. Additionally, we will load an assessments dataset, which provides data on students’ performance on various assessments throughout their courses. students &lt;- read_csv(&quot;data/ml/oulad-students.csv&quot;) ## Rows: 32593 Columns: 15 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (9): code_module, code_presentation, gender, region, highest_education, ... ## dbl (6): id_student, num_of_prev_attempts, studied_credits, module_presentat... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. assessments &lt;- read_csv(&quot;data/ml/oulad-assessments.csv&quot;) ## Rows: 173912 Columns: 10 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): code_module, code_presentation, assessment_type ## dbl (7): id_assessment, id_student, date_submitted, is_banked, score, date, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 14.5.1 Preprocessing and Feature Engineering Our goal is to build a model that predicts whether a student is at risk of dropping out. We will handle some feature engineering directly, while moving a couple of preprocessing steps into the recipe to demonstrate how recipes work. 14.5.1.1 Step 1: Creating Outcome and Predictor Variables Outside the Recipe To begin, we create the outcome variable (pass) and a factor variable for disability directly using mutate(): students &lt;- students %&gt;% mutate(pass = ifelse(final_result == &quot;Pass&quot;, 1, 0)) %&gt;% mutate(pass = as.factor(pass), disability = as.factor(disability)) We will also summarize assessment data to create a new predictor based on students’ performance on assessments submitted early in the course. Specifically, we will calculate the mean weighted score of assessments submitted before the first half of assignment dates. code_module_dates &lt;- assessments %&gt;% group_by(code_module, code_presentation) %&gt;% summarize(quantile_cutoff_date = quantile(date, probs = .5, na.rm = TRUE)) ## `summarise()` has grouped output by &#39;code_module&#39;. You can override using the ## `.groups` argument. assessments_joined &lt;- assessments %&gt;% left_join(code_module_dates) %&gt;% filter(date &lt; quantile_cutoff_date) %&gt;% mutate(weighted_score = score * weight) %&gt;% group_by(id_student) %&gt;% summarize(mean_weighted_score = mean(weighted_score, na.rm = TRUE)) ## Joining with `by = join_by(code_module, code_presentation)` Last, we will create a socioeconomic status variable (imd_band) outside the recipe for clarity: students &lt;- students %&gt;% mutate(imd_band = factor(imd_band, levels = c(&quot;0-10%&quot;, &quot;10-20%&quot;, &quot;20-30%&quot;, &quot;30-40%&quot;, &quot;40-50%&quot;, &quot;50-60%&quot;, &quot;60-70%&quot;, &quot;70-80%&quot;, &quot;80-90%&quot;, &quot;90-100%&quot;))) %&gt;% mutate(imd_band = as.factor(imd_band)) In the OULAD documentation, this is called the VLE (virtual learning environment) data source. Please review the description of the variables in the studentVLE and VLE sources (which are joined together for this learning lab) here. Let’s import it: interactions &lt;- read_csv(&quot;data/ml/oulad-interactions-filtered.csv&quot;) # need to unzip ## Rows: 5548858 Columns: 11 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): code_module, code_presentation, activity_type ## dbl (8): id_student, id_site, date, sum_click, week_from, week_to, module_pr... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 14.5.1.2 Your Turn ⤵ First, count() the activity_type variable and sort the resulting output by frequency. interactions %&gt;% count(activity_type) ## # A tibble: 19 × 2 ## activity_type n ## &lt;chr&gt; &lt;int&gt; ## 1 dataplus 311 ## 2 dualpane 7306 ## 3 externalquiz 18171 ## 4 forumng 1279917 ## 5 glossary 9630 ## 6 homepage 832424 ## 7 htmlactivity 6562 ## 8 oucollaborate 25861 ## 9 oucontent 1065736 ## 10 ouelluminate 13829 ## 11 ouwiki 66413 ## 12 page 33539 ## 13 questionnaire 16528 ## 14 quiz 398966 ## 15 repeatactivity 6 ## 16 resource 436704 ## 17 sharedsubpage 103 ## 18 subpage 1104279 ## 19 url 232573 What does this tell you? Consulting the codebook and your output, please add at least two notes on what you are noticing: Then, let’s create a histogram of the date variable. interactions %&gt;% ggplot(aes(x = date)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Let’s join the data together. students &lt;- students %&gt;% left_join(assessments_joined, by = &quot;id_student&quot;) Next, we’ll load a new file — one with interactions (or log-trace) data. We have to do the same processing we did in the second learning lab, to obtain cut-off dates. As a reminder, the purpose of this is to train the model on data from the first one-third of the class, with the reasoning being this is a good time to intervene–far enough into the class to make an appreciable impact, but not too late to have a limited chance of being able to change students’ trajectory in the class. We’ll repeat the procedure we carried out with the assessments data — calculating a cut-off for each class and then filtering the data based upon this cut-off. But, since we’ve already done this for the assessment data, to allow us to focus more on the modeling, we are providing you with the already-filtered interactions data. Next, we load oulad-interactions-filtered.csv into R, assigning the resulting data frame the name interactions. 14.5.1.3 Your Turn ⤵ interactions &lt;- read_csv(&quot;data/ml/oulad-interactions-filtered.csv&quot;) ## Rows: 5548858 Columns: 11 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): code_module, code_presentation, activity_type ## dbl (8): id_student, id_site, date, sum_click, week_from, week_to, module_pr... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. How can we create a feature with sum_click? Think back to our discussion in the presentation; we have many options for working with such time series data. Perhaps the most simple is to count the clicks. Please summarize the number of clicks for each student (specific to a single course). This means you will need to group your data by id_student, code_module, and code_presentation, and then create a summary variable. interactions_summarized &lt;- interactions %&gt;% group_by(id_student, code_module, code_presentation) %&gt;% summarize(sum_clicks = sum(sum_click)) ## `summarise()` has grouped output by &#39;id_student&#39;, &#39;code_module&#39;. You can ## override using the `.groups` argument. interactions_summarized ## # A tibble: 29,160 × 4 ## # Groups: id_student, code_module [28,192] ## id_student code_module code_presentation sum_clicks ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 6516 AAA 2014J 999 ## 2 8462 DDD 2013J 516 ## 3 8462 DDD 2014J 10 ## 4 11391 AAA 2013J 528 ## 5 23629 BBB 2013B 84 ## 6 23698 CCC 2014J 503 ## 7 23798 BBB 2013J 277 ## 8 24186 GGG 2014B 118 ## 9 24213 DDD 2014B 642 ## 10 24391 GGG 2013J 424 ## # ℹ 29,150 more rows How many times did students click? Let’s create a histogram to see. Please use {ggplot} and geom_histogram() to visualize the distribution of the sum_clicks variable you just created. interactions_summarized %&gt;% ggplot(aes(x = sum_clicks)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. This is a good start - we’ve created our first feature based upon the log data, sum_clicks! What are some other features we can add? An affordance of using the summarize() function in R is we can create multiple summary statistics at once. Let’s also calculate the standard deviation of the number of clicks as well as the mean. Please copy the code you wrote above into the code chunk below and then add these two additional summary statistics. interactions_summarized &lt;- interactions %&gt;% group_by(id_student, code_module, code_presentation) %&gt;% summarize(sum_clicks = sum(sum_click), sd_clicks = sd(sum_click), mean_clicks = mean(sum_click)) ## `summarise()` has grouped output by &#39;id_student&#39;, &#39;code_module&#39;. You can ## override using the `.groups` argument. Let’s join together all of the data we’ll use for our modeling: students and intteractions_summarized. Use left_join() once more, assigning the resulting output the name students_and_interactions. Lots of joining! Sometimes, the hardest part of complex analyses lies in the preparation (and joining) of the data. 14.5.1.4 Your Turn ⤵ students_and_interactions &lt;- left_join(students, interactions_summarized) ## Joining with `by = join_by(code_module, code_presentation, id_student)` 14.5.1.5 Step 2: Splitting the Data We split the dataset into training and testing sets using an 80-20 split: set.seed(2025) train_test_split &lt;- initial_split(students_and_interactions, prop = 0.8, strata = &quot;pass&quot;) data_train &lt;- training(train_test_split) data_test &lt;- testing(train_test_split) 14.5.1.6 Step 3: Creating a Recipe for Selected Preprocessing Steps To keep things simple, we will only include a few pre-processing steps. my_rec &lt;- recipe(pass ~ disability + imd_band + mean_weighted_score + num_of_prev_attempts + gender + region + highest_education + sum_clicks + sd_clicks + mean_clicks, data = data_train) %&gt;% # steps step_impute_mean(mean_weighted_score, sum_clicks, sd_clicks, mean_clicks) %&gt;% step_impute_mode(imd_band) %&gt;% step_center(mean_weighted_score) %&gt;% step_scale(mean_weighted_score) %&gt;% step_dummy(all_nominal_predictors(), -all_outcomes()) %&gt;% step_scale(num_of_prev_attempts) We can inspect the recipe to verify the steps: my_rec ## ## ── Recipe ────────────────────────────────────────────────────────────────────── ## ## ── Inputs ## Number of variables by role ## outcome: 1 ## predictor: 10 ## ## ── Operations ## • Mean imputation for: mean_weighted_score, sum_clicks, sd_clicks, ... ## • Mode imputation for: imd_band ## • Centering for: mean_weighted_score ## • Scaling for: mean_weighted_score ## • Dummy variables from: all_nominal_predictors() and -all_outcomes() ## • Scaling for: num_of_prev_attempts 14.5.1.7 Step 4: Specifying the Model and Workflow Next, we specify a logistic regression model and bundle the recipe and model into a workflow. This step has a lot of pieces, but they are fairly boilerplate. The first part is to specify the model: my_mod &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) # my_mod &lt;- boost_tree() %&gt;% # set_engine(&quot;xgboost&quot;) %&gt;% # &quot;xgboost&quot; is a common engine for boosted trees # set_mode(&quot;classification&quot;) The next is to specify the workflow. my_wf &lt;- workflow() %&gt;% add_recipe(my_rec) %&gt;% add_model(my_mod) 14.5.1.8 Step 5: Fitting the Model Now we fit the model to the training data. We do this by calling the last_fit() function on the workflow: final_fit &lt;- last_fit(my_wf, train_test_split) 14.5.1.9 Step 6: Evaluating Model Performance Finally, we evaluate the model’s performance using the test set. The tidymodels package makes this easy. metrics &lt;- final_fit %&gt;% collect_metrics() metrics ## # A tibble: 3 × 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 accuracy binary 0.640 Preprocessor1_Model1 ## 2 roc_auc binary 0.639 Preprocessor1_Model1 ## 3 brier_class binary 0.224 Preprocessor1_Model1 So, how did the model do? Not so bad! One affordance of tidymodels is we can readily switch out the model and engine. Try one of these modifications to the code to see how the predictive performance improves—for a random forest and a boosted tree model, respectively. my_mod &lt;- rand_forest() %&gt;% set_engine(&quot;ranger&quot;) %&gt;% # install.packages(&quot;ranger&quot;) needed first set_mode(&quot;classification&quot;) my_mod &lt;- boost_tree() %&gt;% # install.packages(&quot;xgboost&quot;) needed first set_engine(&quot;xgboost&quot;) %&gt;% set_mode(&quot;classification&quot;) 14.6 Conclusion Though we focus on this relatively simple model, or algorithm,, many of the ideas explored in this chapter will likely extend and prove useful for other machine learning methods. Our goal is for you to finish this final walkthrough with the confidence to explore using machine learning to answer a question or to solve a problem of your own with respect to teaching, learning, and educational systems. In this chapter, we introduced general machine learning ideas, like training and test datasets and evaluating the importance of specific variables, in the context of predicting students’ passing a course. Like many of the topics in this book—but, perhaps particularly so for machine learning—there is much more to discover on the topic, and we encourage you to consult the books and resources in the Learning More chapter to learn about further applications of machine learning methods. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
