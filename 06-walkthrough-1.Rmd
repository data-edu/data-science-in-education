---
title: 'Education Dataset Analysis Pipeline: Walkthrough #1'
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = F)
```

# Background 

In the 2015-2016 and 2016-2017 school years, researchers at Michigan State University carried out a study on students' motivation to learn in online science classes. The online science classes were part of a statewide online course provider designed to supplement (and not replace) students' enrollment in their local school. For example, students may choose to enroll in an online physics class because one was not offered at their school (or they were not able to take it given their schedule).

The study involved a number of different data sources which were brought to bear to understand students' motivation:

1. A self-report survey for three distinct but related aspects of students' motivation
2. Log-trace data, such as data output from the learning management system
3. Achievement-related and gradebook data
4. Discussion board data
5. Achievement-related (i.e., final grade) data

First, these different data sources will be described in terms of how they were provided by the school.

## 1. Self-report survey 

This was data collected before the start of the course via self-report survey. The survey included 10 items, each corresponding to one of three *measures*, namely, for interest, utility value, and perceived competence:

1.	I think this course is an interesting subject. (Interest)
2.	What I am learning in this class is relevant to my life. (Utility value)
3.	I consider this topic to be one of my best subjects. (Perceive competence)
4.	I am not interested in this course. (Interest - reverse coded)
5.	I think I will like learning about this topic. (Interest)
6.	I think what we are studying in this course is useful for me to know. (Utility value)
7.	I don’t feel comfortable when it comes to answering questions in this area. (Perceived competence)
8.	I think this subject is interesting. (Interest)
9.	I find the content of this course to be personally meaningful. (Utility value)
10.	I’ve always wanted to learn more about this subject. (Interest)

## 2. Log-trace data 

Log-trace data is data generated from our interactions with digital technologies. In education, an increasingly common source of log-trace data is that generated from interactions with learning management systems. The data for this walk-through is a *summary of* log-trace data, namely, the number of minutes students spent on the course. Thus, while this data is rich, you can imagine even more complex sources of log-trace data (i.e. timestamps associated with when students started and stopped accessing the course!).

## 3. Achievement-related and gradebook data

This is a common source of data, namely, one associated with graded assignments students completed. In this walkthrough, we just examine students' final grade.

## 4. Discussion board data
<!-- NOTE - may not include this, as it is hard to confidently anonymize a medium-ish sized data set -->

Discussion board data is both rich and unstructured, in that it is primarily in the form of written text. We examine a small subset of the discussion board data in this walkthrough.

# Processing the data

This analysis uses R packages, which are collections of R code that help users code more efficiently. We load these packages with the function `library`. In particular, the packages we'll use will help us load Excel files, organize the structure of the data, work with dates in the data, and navigate file directories. 

```{r, loading-packages}
library(readxl)
library(tidyverse)
library(lubridate)
library(here)
```

This code chunk loads the log trace data using the `read_csv` function. Note that we call `read_csv` three times, once for each of the three logtrace datasets. We assign each of the datasets a name using `<-`.

```{r}
# Gradebook and log-trace data for F15 and S16 semesters
s12_course_data <- read_csv(
  here(
    "data", 
    "online-science-motivation", 
    "raw", 
    "s12-course-data.csv"
  )
)

# Pre-survey for the F15 and S16 semesters
s12_pre_survey  <- read_csv(
  here(
    "data", 
    "online-science-motivation", 
    "raw", 
    "s12-pre-survey.csv"
  )
) 

# Log-trace data for F15 and S16 semesters - ts is for time spent
s12_time_spent <- read_csv(
  here(
    "data", 
    "online-science-motivation", 
    "raw", 
    "s12-course-minutes.csv"
  )
)
```

## Viewing the data

Now that we've successfully loaded all three logtrace datasets, we can visually inspect the data by typing the names that we assigned to each dataset. 

```{r}
s12_pre_survey 
s12_course_data
s12_time_spent
```

## Processing the pre-survey data

Often, survey data needs to be processed in order to be (most) useful. Here, we process the self-report items into three scales, for: interest, self-efficacy, and utility value. We do this by 

 - Renaming the question variables to something more managable 
 - Reversing the response scales on questions 4 and 7 
 - Categorizing each question into a measure 
 - Computing the mean of each measure 

Let's take these steps in order: 

1. Rename the question columns to something much simpler: 

```{r}
s12_pre_survey  <- s12_pre_survey  %>%
    # Rename the qustions something easier to work with because R is case sensitive
    # and working with variable names in mix case is prone to error
    rename(q1 = Q1MaincellgroupRow1,
           q2 = Q1MaincellgroupRow2,
           q3 = Q1MaincellgroupRow3,
           q4 = Q1MaincellgroupRow4,
           q5 = Q1MaincellgroupRow5,
           q6 = Q1MaincellgroupRow6,
           q7 = Q1MaincellgroupRow7,
           q8 = Q1MaincellgroupRow8,
           q9 = Q1MaincellgroupRow9,
           q10 = Q1MaincellgroupRow10) %>% 
    # Convert all question responses to numeric
    mutate_at(vars(q1:q10), funs(as.numeric))
```

2. Next we'll reverse the scale of the survey responses on questions 4 and 7 so the responses for all questions can be interpreted in the same way. Rather than write a lot of code once to reverse the scales for question 4 then writing it again to reverse the scales on question 7, we'll build a function that does that job for us. Then we'll use the same function for question 4 and question 7. This will result in much less code, plus it will make it easier for us to change in the future. 

```{r}
# This part of the code is where we write the function:

# Function for reversing scales 
reverse_scale <- function(question) {
    # Reverses the response scales for consistency 
    #   Args: 
    #     question: survey question 
    #   Returns: a numeric converted response
  # Note: even though 3 is not transformed, case_when expects a match for all
  # possible conditions, so it's best practice to label each possible input
  # and use TRUE ~ as the final statement returning NA for unexpected inputs
  x <- case_when(question == 1 ~ 5, 
                 question == 2 ~ 4,
                 question == 4 ~ 2,
                 question == 5 ~ 1,
                 question == 3 ~ 3,
                 TRUE ~ NA_real_)
  x
}

# And here's where we use that function to reverse the scales

# Reverse scale for questions 4 and 7
s12_pre_survey <- s12_pre_survey %>%
  mutate(q4 = reverse_scale(q4),
         q7 = reverse_scale(q7))
```

3. We'll accomplish the last two steps in one chunk of code. First we'll create a column called `measure` and we'll fill that column with one of three question categories: 

 - `int`: interest
 - `uv`: utility value 
 - `pc`: self efficacy

After that we'll find the mean response of each category using `mean` function.

```{r}
# Add measure variable 
s12_measure_mean <- s12_pre_survey %>% 
    # Gather questions and responses 
    gather(question, response, c(q1:q10)) %>% 
    mutate(
        # Here's where we make the column of question categories 
        measure = case_when(
            question %in% c("q1", "q4", "q5", "q8", "q10") ~ "int", 
            question %in% c("q2", "q6", "q9") ~ "uv", 
            question %in% c("q3", "q7") ~ "pc", 
            TRUE ~ NA_character_
        )) %>% 
    group_by(measure) %>% 
    summarise(
        # Here's where we compute the mean of the responses 
        # Mean response for each measure
        mean_response = mean(response, na.rm = TRUE), 
        # Percent of each measure that had NAs in the response field
        percent_NA = mean(is.na(response))
        )

s12_measure_mean
```

## Processing the course data

We also can process the course data in order to obtain more information.

```{r}
# split course section into components
s12_course_data <- s12_course_data %>%
  separate(col = 1,
           into = c('subject', 'semester', 'section'),
           sep = '-',
           remove = FALSE)

```

This led to pulling out the subject, semester, and section from the course ID; variables that we can use later on.

## Joining the data

To join the course data and pre-survey data, we need to create similar *keys*. In other words, our goal here is to have one variable that matches across both datasets, so that we can merge the datasets on the basis of that variable. 

For these data, both have variables for the course and the student, though they have different names in each. Our first goal will be to rename two variables in each of our datasets so that they will match. One variable will correspond to the course, and the other will correspond to the student. We are not changing anything in the data itself at this step - instead, we are just cleaning it up so that we can look at the data all in one place.

Let's start with the pre-survey data. We will rename RespondentID and opdata_CourseID to be student_id and course_id, respectively.

```{r}
s12_pre_survey <- s12_pre_survey %>% 
    rename(student_id = RespondentId,
           course_id = opdata_CourseID)

s12_pre_survey
```

Looks better now!

Let's proceed to the course data. Our goal is to rename two variables that correspond to the course and the student so that we can match with the other variables we just created for the pre-survey data.

```{r}
s12_course_data <- s12_course_data %>% 
    rename(student_id = Bb_UserPK,
           course_id = CourseSectionOrigID)

s12_course_data
```

Now that we have two variables that are consistent across both datasets - we have called them "course_id" and "student_id" -  we can join these using the **dplyr** function, `left_join()`. 
Let's save our joined data as a new object called "dat."

```{r}
dat <- left_join(s12_course_data, 
                 s12_pre_survey, 
                 by = c("student_id", "course_id"))

dat
```

Just one more data frame to merge:

```{r}
s12_time_spent <- s12_time_spent %>%
  rename(student_id = Bb_UserPK, 
         course_id = CourseSectionOrigID)
s12_time_spent <- s12_time_spent %>%
  mutate(student_id = as.integer(student_id))
dat <- dat %>% 
  left_join(s12_time_spent, 
            by = c("student_id", "course_id"))
```

Note that they're now combined, even though the course data has many more rows: The pre_survey data has been joined for each student by course combination.

We have a pretty large data frame! Let's take a quick look.

```{r}
dat
```

It looks like we have neary 30,000 observations from 30 variables.

Now that our data are ready to go, we can start to ask some questions of the data. 

# Visualizations and Models

One thing we might be wondering is how time spent on course is related to students' final grade. Let's first calculate the percentage of points students earned as a measure of their final grade (noting that the teacher may have assigned a different grade--or weighted their grades in ways not reflected through the points).

```{r}
dat <- dat %>% 
    group_by(student_id, course_id) %>% 
    mutate(Points_Earned = as.integer(Points_Earned)) %>% 
    summarize(total_points_possible = sum(Points_Possible, na.rm = TRUE),
              total_points_earned = sum(Points_Earned, na.rm = TRUE)) %>% 
    mutate(percentage_earned = total_points_earned/total_points_possible) %>% 
    ungroup() %>% 
    left_join(dat) # note that we join this back to the original data frame to retain all of the variables
```

## Visualization of the relationship between time spent on course and percentage of points earned

<!-- This is really trivial and obvious; need a new/better relationship -->

```{r}
ggplot(dat, aes(x = TimeSpent, y = percentage_earned)) +
    geom_point()
```

There appears to be *some* relationship. What if we added a line of best fit - a linear model?

```{r}
ggplot(dat, aes(x = TimeSpent, y = percentage_earned)) +
    geom_point() + 
    geom_smooth(method = "lm")
```

So, it appeares that the more time students spent on the course, the more points they earned.

# Linear model (regression)

We can find out exactly what the relationship is using a linear model.

```{r}
m_linear <- lm(percentage_earned ~ TimeSpent, data = dat)
summary(m_linear)
```

## But what about different courses?

Is there course-specific differences in how much time students spend on the 
course as well as in how time spent is related to the percentage of points 
students earned?

```{r}
ggplot(dat, aes(x = TimeSpent, y = percentage_earned, color = course_id)) +
    geom_point()
```

```{r}
ggplot(dat, aes(x = TimeSpent, y = percentage_earned, color = course_id)) +
    geom_point() +
    geom_smooth(method = "lm")
```

There appears to be so. One way we can test is to use what is called a multi-level model. This requires a new package; one of the most common for estimating these types of models is **lme4**. We use it very similarly to the `lm()` function, but we pass it an additional argument about what the *groups*, or levels, in the data are.

```{r}
# install.packages("lme4")
library(lme4)
m_course <- lmer(percentage_earned ~ TimeSpent + (1|course_id), data = dat)
summary(m_course)
```

A common way to understand how much variability is at the group level is to calculate the *intra-class* correlation. This value is the proportion of the variability in the outcome (the *y*-variable) that is accounted for solely by the groups identified in the model. There is a useful function in the **sjstats** package for doing this.

```{r}
# install.packages("sjstats")
library(sjstats)
icc(m_course)
```

This shows that nearly 17% of the variability in the percentage of points students earned can be explained simply by knowing what class they are in. 
