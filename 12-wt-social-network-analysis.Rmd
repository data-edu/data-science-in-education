# Walkthrough 6: Exploring relationships using social network analysis with social media data {#c12}

<!-- TODO -->

<!-- framing digital and physical, and when to use -->

<!-- framing different interactions, and how to choose -->

<!-- finish coding and explaining -->

**Abstract**

This chapter explores the use of social network analysis (commonly referred to as SNA). While much of the data that data scientists in education analyze pertains to variables for individuals, some data concerns the relationships between individuals, such as friendship between youth, or advice-seeking on the part of teachers. While such data is common---and often of interest to data scientists---it can be difficult to analyze, in part due to the multiple sources of data (about both individuals’ relations and individuals and the complexity of the relations between individuals. This chapter uses data from the Bluesky #tidyuesday network, an R learning community, to demonstrate how such data can be accessed and imported, processed into forms necessary for social network analysis using the {tidygraph} R package, and visualized using the {ggraph} R package.

## Topics emphasized

-   Transforming data
-   Visualizing data

## Functions introduced

-   `bskyr::bs_search_posts()`
-   `randomNames::randomNames()`
-   `tidyr::unnest()`
-   `tidygraph::as_tbl_graph()`
-   `ggraph::ggraph()`

## Vocabulary

-   Application Programming Interface (API)
-   edgelist
-   edge
-   influence model
-   regex
-   selection model
-   social network analysis
-   sociogram
-   vertex

## Chapter overview

In education, we are used to focusing on *individuals* – students and teachers, especially. We can learn many things from studying individual students or teachers, but, sometimes, what is interesting lies beyond them–or *between* individuals.

Social network analysis is useful for understanding the relationships between individuals.

For instance, social network analyses can be useful for asking questions about *social influence* [@frank1998chapter]:

-   How does the pattern of interactions among teachers in a professional network explain their continued involvement in the network? [@rosenberg2021advancing]

We can also ask questions about *social selection*,

-   Which parents are the most central

In this chapter, we consider questions like these

In this, we chapter focus on the *interactions* between #TidyTuesday participants using social network analysis (sometimes simply referred to as network analysis) techniques.

While social network analysis is increasingly common, it can be hard. Cleaning and tidying the data, especially, can be trickty.can be even more challenging than for most other data sources because net data for social network analysis (or network data) often includes variables about both individuals (such as information students or teachers) and their relationships (whether they have a relationship at all, for example, or how strong or of what type their relationship is).

This chapter is designed to take you from not having carried out social network analysis through visualizing social media data (though, as we describe next, we can also use self-report survey and other types of data, as well).

For those wishing to dive deeper, we've included an appendix ([Appendix C](#c20c)) to introduce some social network-related ideas for further exploration; these focus on modeling social network processes, particularly, the processes of who chooses (or selects) to interact with whom, and of influence, or how relationships can impact individuals' behaviors.

One last note. You will need a Bluesky account to complete *all* of the code outlined in this chapter in order to access your own Bluesky data (through the Bluesky Application Interface \[API\]). If you do not have a Bluesky account, you can create one and keep it private, or even delete the account once you're done with this walkthrough.

## Background

Traditionally, social network analysis was carried out using self-report surveys. An example of this is XX

More recently, researchers and analysts have begun other data sources---especially data from digital tools, including social media [@rosenberg-et-al-2025].

There are a few things to know before getting started. First, as with most social media platforms, Bluesky places some restrictions on accessing its Application Programming Interface (API). An API is a set of structured http requests that return data in a predictable way. These restrictions involve the number of posts that can be accessed using the API. Bluesky's rate limits are generally generous for typical educational research and #TidyTuesday analysis.

Occasionally, Bluesky will make changes to its API that affect how the `bskyr` package works. When this happens, you may need to wait for the package to be updated. It is a good idea to install the latest version of the package before beginning a new project by running `install.packages("bskyr")` or checking for updates.

In the past, if a teacher wanted advice about how to plan a unit or to design a lesson, they would turn to a trusted peer in their building or district [@spillane2012]. Today they are as likely to turn to someone in a social media network. Social media interactions like the ones tagged with the #tidytuesday hashtag are increasingly common in education. Using data science tools to learn from these interactions is valuable for improving the student experience.

## Packages

In this chapter, we access data using the {bskyr} package [@bskyr], providing access to data from the Bluesky social media platform. In the first edition of this book, we used a different data sources---Twitter (X) posts. Due to changes in the accessibility of that data (specifically, that it is now *very* difficult to access it), we have switched to a different social media platform, Bluesky.

Fortunately, it is about as easy to access data from Bluesky as it *was* to access data from Twitter. In addition to the {bskyr} package, we'll load a few other packages that we will be using in this analysis, including two packages related to social network analysis [@R-tidygraph; @R-ggraph] as well as one that will help us to use not-anonymized names in a savvy way [@R-randomNames]. As always, if you have not installed any of these packages before (which may particularly be the case for the {bskyr}, {randomNames}, {tidygraph}, and {ggraph} packages, which we have not yet used in the book), do so using the `install.packages()` function. More on installing packages is included in the ["*Packages*"](#c06p) section of the ["*Foundational Skills*"](#c06) chapter.

Let's load the packages with the following calls to the `library()` function:

```{r, message = F, warning = F}
library(tidyverse)
library(bskyr)
# library(dataedu)

library(randomNames)
library(tidygraph)
library(ggraph)
library(igraph)

```

## Data sources and import

The first step in the use of the {bskyr} package is to gain access ot the Application Programming Interface.

To do so, you'll need to visit <https://bsky.app/settings> in a browser.

Then, go to "Privacy and Security", and click "App passwords" and "Add App Password". You *do not* need to check, "Allow access to your direct messages". A code will pop up (see the figure below) that you will copy and use next.

Almost authenticated! You'll run these two lines of code *once*. This saves your username and this passcode in a hidden file (called \`.Renviron\`). That you will use for *all* subsequent analyses.

```{r, eval=FALSE}
set_bluesky_user('YOUR-USERNAME.bsky.social') # your username goes here -- including .bky.social at the end!
set_bluesky_pass('your-apps-pass-word') # the passcode you just copied goes here
```

The next line is one you'll run *every time* you

```{r, eval = FALSE}
auth <- bs_auth(user = bs_get_user(), pass = bs_get_pass())
```

We'll next collect some tweets --- for the #tidytuesday hashtag on Bluesky.

Let's start with one month – January, 2025. We need to specify a few things:

-   the *query*

-   the *date range*

-   the *maximum number of posts we want to collect*

-   our *authentication* information

```{r, eval = FALSE}
posts_jan <- bs_search_posts(query = "#tidytuesday", 
                             since = '2025-01-01T00:00:00.000Z', until = '2025-01-31T23:59:59.000Z',
                             limit = 10000,
                             auth = auth)
```

Let's inspect the result:

```{r}
posts_jan %>% 
    glimpse()
```

Let's continue to collect four more months.

```{r, eval = FALSE}
posts_feb <- bs_search_posts(query = "#tidytuesday", 
                             since = '2025-02-01T00:00:00.000Z', until = '2025-02-28T23:59:59.000Z',
                             limit = 10000,
                             auth = auth)


posts_mar <- bs_search_posts(query = "#tidytuesday", 
                             since = '2025-03-01T00:00:00.000Z', until = '2025-03-31T23:59:59.000Z',
                             limit = 10000,
                             auth = auth)

posts_apr <- bs_search_posts(query = "#tidytuesday", 
                             since = '2025-04-01T00:00:00.000Z', until = '2025-04-30T23:59:59.000Z',
                             limit = 10000,
                             auth = auth)

posts_may <- bs_search_posts(query = "#tidytuesday", 
                             since = '2025-05-01T00:00:00.000Z', until = '2025-05-31T23:59:59.000Z',
                             limit = 10000,
                             auth = auth)
```

Now, we can combine these data frames together and assign the results to a single object, `posts`, and then look at the results:

```{r, eval = FALSE}
bind_rows(posts_jan, posts_feb, posts_mar, posts_apr, posts_may) -> posts

posts %>% 
    glimpse()
```

```{r}
# write_rds(posts, "posts.rds")
posts <- read_rds("posts.rds")
```

You can easily change the search term to other hashtags terms. For example, to search for #rstats posts, we can replace #tidytuesday with #rstats.

```{r, eval = FALSE}
rstats_posts <- bs_search_posts(query = "#rstats", 
                                since = '2024-12-01T00:00:00.000Z', 
                                until = '2024-12-31T23:59:59.000Z',
                                limit = 10000,
                                auth = auth)
```

You may notice that the most recent posts containing the #tidytuesday hashtag are returned. What if you wanted to go further back in time? We’ll discuss this topic in the next section and in Appendix B.

## Using an Application Programming Interface (or API)

It's worth taking a short detour to talk about how you can obtain a dataset spanning a longer period of time. A common way to import data from websites, including social media platforms, is to use something called an Application Programming Interface (API). In fact, if you ran the code above, you just accessed an API!

Think of an API as a special door a home builder made for a house that has a lot of cool stuff in it. The home builder doesn’t want everyone to be able to walk right in and use a bunch of stuff in the house. But they also don’t want to make it too hard because, after all, sharing is caring! Imagine the home builder made a door just for folks who know how to use doors. In order to get through this door, users need to know where to find it along the outside of the house. Once they’re there, they have to know the code to open. And, once they’re through the door, they have to know how to use the stuff inside. An API for social media platforms like Twitter and Facebook are the same way. You can download datasets of social media information, like tweets, using some code and authentication credentials organized by the website.

There are some advantages to using an API to import data at the start of your education dataset analysis. Every time you run the code in your analysis, you’ll be using the API to contact the social media platform and download a fresh dataset. Now your analysis is not just a one-off product, but, rather, is one that can be updated with the most recent data (in this case, Tweets), every time you run it. By using an API to import new data every time you run your code, you create an analysis that can be used again and again on future datasets. However, a key point---and limitation---is that Twitter allows access to their data via their API only for (approximately) the seven most recent days. There are a number of *other* ways to access older data, though we focus on one way here: having access to the URLs of (or the status IDs for) tweets.

As a result, we used this technique---described in-depth in [Appendix B](#c20b)---to collect older (historical) data from Twitter about the #tidytuesday hashtag, using a different function than the one described above (`rtweet::lookup_statuses()` instead of `rtweet::search_tweets()`). This was important for this chapter because having access to a greater number of tweets allows us to better understand the interactions between a larger number of the individuals participating in #tidytuesday. The data that we prepared from accessing historical data for #tidytuesday is available in the {dataedu} R package as the `tt_tweets` dataset, as we describe next.

## Processing the Data

This data is complex!

If we take a glimpse at `posts`, we see that many of the columns are "named list"s:

```{r}
posts %>% 
    glimpse()
```

What does this "named list" variable type mean? When we look at what we think is the author variable, we can see it's, in fact, more complex. The reason is that the Bluesky API returns data in a format called JSON (JavaScript Object Notation), which isn't always compatible with the *table* (think spreadsheet) data we are used to. The author itself contains many things we could call variables. Take a look:

```{r}
posts$author %>% 
    pluck(14)
```

Even with this complex "named list" data type, this data *can* be shaped up into a table. We'll show how we can do that in a moment.

First, though, we note that there's a similar situation for the field called "record", which includes the time stamp for the post, its text (i.e., the contents of the post), languages specified, and some interesting meta-data, including whether hashtags are included, mentions of other users are included, or whether the post is a reply.

We won't run the following code, as the output is quite lengthy, but please try to run it (just make sure you've loaded the data, `posts`, earlier)!

```{r, eval = FALSE}
posts$record %>% 
    pluck(1)
```

### What *is* our network?

Recall the discussion at the beginning of this chapter on the wide variety in the types of networks

We need the author's handle to identify them as a node in our network. But we can't easily use it while it's buried inside this list structure.

## View data

We can see that there are *quite a few rows* rows for the data:

```{r}
nrow(posts)
```

## Methods: process data

Network data requires some processing before it can be used in subsequent analyses. The network dataset needs a way to identify each participant's role in the interaction. We need to answer questions like: Did someone reach out to another for help? Was someone contacted by another for help? We can process the data by creating an "edgelist". An edgelist is a dataset where each row is a unique interaction between two parties. Each row (which represents a single relationship) in the edgelist is referred to as an "edge". We note that one challenge facing data scientists beginning to use network analysis is the different terms that are used for similar (or the same!) aspects of analyses: Edges are sometimes referred to as "ties" or "relations", but these generally refer to the same thing, though they may be used in different contexts.

An edgelist looks like the following, where the `sender` (sometimes called the "nominator") column identifies who is initiating the interaction and the `receiver` (sometimes called the "nominee") column identifies who is receiving the interaction:

```{r, include = FALSE}
names_d1 <-
    randomNames(6) %>%
    enframe(name = NULL) %>%
    mutate(sender = 1:6) %>%
    set_names(c("sender2", "sender"))

names_d2 <-
    randomNames(6) %>%
    enframe(name = NULL) %>%
    mutate(receiver = 1:6) %>%
    set_names(c("receiver2", "receiver"))

example_edgelist <-
    tibble(
        sender = c(2, 1, 3, 1, 2, 6, 3, 5, 6, 4, 3, 4),
        receiver = c(1, 2, 2, 3, 3, 3, 4, 4, 4, 5, 6, 6)
    )

example_edgelist <-
    example_edgelist %>%
    left_join(names_d1) %>%
    left_join(names_d2) %>%
    select(sender = receiver2, receiver = sender2)
```

```{r, echo = FALSE}
example_edgelist
```

In this edgelist, the `sender` column might identify someone who nominates another (the receiver) as someone they go to for help. The sender might also identify someone who interacts with the receiver in other ways, like "liking" or "mentioning" their posts. In the following steps, we will work to create an edgelist from the data from #tidytuesday on Bluesky.

### Extracting nodes and edges

Now we need to extract two key pieces of information from our `posts` data:

1.  **Nodes**: The unique individuals (users) in our network
2.  **Edges**: The interactions between users (replies and mentions)

First, we'll define a helper function to safely extract data from the nested list structures in the Bluesky data:

```{r}
pluck_chr <- function(x, ..., .default = NA_character_) {
  purrr::pluck(x, ..., .default = .default) %>% as.character()
}

# Extract nodes (unique users) from posts
nodes <- posts %>%
  mutate(
    did = map_chr(author, ~pluck_chr(.x, "did")),
    handle = map_chr(author, ~pluck_chr(.x, "handle")),
    display_name = map_chr(author, ~pluck_chr(.x, "displayName"))
  ) %>%
  distinct(did, .keep_all = TRUE) %>%
  transmute(
    did = did,
    label = coalesce(display_name, handle, did)
  )

# Extract mention edges from posts
# First, get reply edges (when someone replies to another user)
reply_edges <- posts %>%
  mutate(
    from = map_chr(author, ~pluck_chr(.x, "did")),
    to = map_chr(reply, ~pluck_chr(.x, "parent", "author", "did"))
  ) %>%
  filter(!is.na(to), to != "") %>%
  select(from, to)

# Second, get mention edges from facets in the post record
mention_facet_edges <- posts %>%
  mutate(
    from = map_chr(author, ~pluck_chr(.x, "did")),
    facets = map(record, ~pluck(.x, "facets", .default = list()))
  ) %>%
  # Expand rows for each facet
  unnest(facets, keep_empty = FALSE) %>%
  # Extract mentioned DIDs from facets
  mutate(
    feature_type = map_chr(facets, ~pluck_chr(.x, "features", 1, "$type")),
    to = map_chr(facets, ~pluck_chr(.x, "features", 1, "did"))
  ) %>%
  filter(feature_type == "app.bsky.richtext.facet#mention", !is.na(to), to != "") %>%
  select(from, to)

# Combine reply and mention edges
mention_edges <- bind_rows(reply_edges, mention_facet_edges) %>%
  distinct()

# 0) Make sure node IDs are unique, non-missing
nodes <- nodes %>%
  filter(!is.na(did), did != "") %>%
  distinct(did, .keep_all = TRUE) %>%
  mutate(label = coalesce(label, did))
```

Let's take a look at what we've created. First, the nodes:

```{r}
nodes %>% 
  glimpse()
```

And the edges (interactions):

```{r}
mention_edges %>% 
  glimpse()
```

Great! Now we have our network data structured properly. The next step is to prepare this data for visualization by converting it into a format that the {tidygraph} and {ggraph} packages can work with.

```{r}
# 1) Build a lookup table: DID -> row index
node_index <- nodes %>%
  mutate(.idx = row_number()) %>%
  select(did, .idx)

# 2) Map edge endpoints to integer indices
edges_idx <- mention_edges %>%
  filter(!is.na(from), from != "", !is.na(to), to != "") %>%
  left_join(node_index, by = c("from" = "did")) %>%
  rename(.from_idx = .idx) %>%
  left_join(node_index, by = c("to" = "did")) %>%
  rename(.to_idx = .idx)

# 3) Diagnose any missing endpoints (these would trigger the NaN-like error)
bad_edges <- edges_idx %>% filter(is.na(.from_idx) | is.na(.to_idx))
if (nrow(bad_edges) > 0) {
  message("Dropping ", nrow(bad_edges), " edges with unknown endpoints.\n",
          "Examples:\n",
          paste(utils::capture.output(print(head(bad_edges, 5))), collapse = "\n"))
}

# Keep only valid edges; (optional) drop self-loops
edges_ok <- edges_idx %>%
  filter(!is.na(.from_idx), !is.na(.to_idx)) %>%
  filter(.from_idx != .to_idx) %>%
  transmute(from = .from_idx, to = .to_idx)

# 4) Build the graph
g_mentions <- tbl_graph(nodes = nodes, edges = edges_ok, directed = TRUE)

g_mentions
```

We've now created a network graph object (`g_mentions`) that contains both our nodes (users) and edges (interactions). This graph is a directed network, meaning that the direction of the interaction matters—who mentioned or replied to whom.



------------------------------------------------------------------------

## Analysis and results

Now that we have our network graph (`g_mentions`), we can analyze and visualize it. We'll use the {tidygraph} and {ggraph} packages to calculate network metrics and create visualizations. We note that network visualizations are often referred to as "sociograms" or a representation of the relationships between individuals in a network. We use this term and the term network visualization interchangeably in this chapter.

### Descriptive statistics

```{r}
g_metrics <- g_mentions %>%
  activate(nodes) %>%
  mutate(
    in_deg  = centrality_degree(mode = "in"),   # mentions received
    out_deg = centrality_degree(mode = "out"),  # mentions made others
    btw     = centrality_betweenness(
                directed = TRUE, normalized = TRUE  # 0-1 scale
              )
  )

nodes_tbl <- g_metrics %>%
  activate(nodes) %>%
  as_tibble() %>%
  mutate(
    in_deg_rank  = min_rank(desc(in_deg)),
    out_deg_rank = min_rank(desc(out_deg)),
    btw_rank     = min_rank(desc(btw))
  )

```

### Plotting the network

Now let's create a visualization of our network. We'll size nodes by their in-degree (how many mentions they received) and color them by betweenness centrality (how important they are for connecting different parts of the network). To avoid clutter, we'll only label the most-mentioned users:

```{r}

# ---- 2) Network plot (size = in-degree, color = betweenness) ----
# Label only the most-mentioned nodes to avoid clutter:
label_threshold <- quantile(nodes_tbl$in_deg, 0.95, na.rm = TRUE)

nodes_tbl <- nodes_tbl %>%
  mutate(lbl = ifelse(in_deg >= label_threshold & in_deg > 0, label, NA))

set.seed(42)

# add a 'lbl' column on nodes
g_metrics <- g_metrics %>%
  activate(nodes) %>%
  mutate(
    lbl = ifelse(
      in_deg >= quantile(in_deg, 0.95, na.rm = TRUE) & in_deg > 0,
      label, NA_character_
    )
  )


p_net <- ggraph(g_metrics, layout = "fr") +          # Fruchterman–Reingold
  geom_edge_link(alpha = 0.8, linewidth = 0.2) +
  geom_node_point(aes(size = in_deg, color = btw), alpha = 0.9) +
  geom_node_text(aes(label = lbl), size = 3, vjust = -0.8, na.rm = TRUE) +
  scale_size_continuous(name = "In-degree (mentions)", range = c(2, 12)) +
  scale_color_continuous(name = "Betweenness", labels = label_percent(accuracy = 0.1)) +
  guides(color = guide_colorbar(barheight = unit(60, "pt"))) +
  theme_void() +
  ggtitle("Mention Network: size = in-degree, color = betweenness")

p_net

```

This visualization shows the structure of the #tidytuesday network on Bluesky. Larger nodes represent users who receive more mentions, and the color indicates their betweenness centrality—how crucial they are in connecting different parts of the network.

### Alternative visualization approach (old)

The code below shows an older approach to network visualization using a simpler filtering method. This is kept for reference.

```{r, old, eval=FALSE}
interactions_sent <- edgelist %>% 
    count(sender) %>% 
    arrange(desc(n))

interactions_sent <- interactions_sent %>% 
    filter(n > 1)

edgelist <- edgelist %>% 
    filter(sender %in% interactions_sent$sender,
           receiver %in% interactions_sent$sender)

g <- as_tbl_graph(edgelist)
```

```{r, old, eval=FALSE, fig.cap="Network Graph"}
g %>%
    ggraph(layout = "kk") +
    geom_node_point() +
    geom_edge_link(alpha = .2) +
    theme_graph()
```

```{r, old, eval=FALSE, echo = FALSE, message=F, warning=F}
extrafont::loadfonts()
```

```{r, old, eval=FALSE, fig.cap="Network Graph with Centrality"}
g %>% 
    mutate(centrality = centrality_authority()) %>% 
    ggraph(layout = "kk") + 
    geom_node_point(aes(size = centrality, color = centrality)) +
    scale_color_continuous(guide = 'legend') + 
    geom_edge_link(alpha = .2) +
    theme_graph()
```

There is much more you can do with {ggraph} (and {tidygraph}); check out the {ggraph} tutorial here: <https://ggraph.data-imaginist.com/>

## Conclusion

In this chapter, we used social media data from the #tidytuesday hashtag on Bluesky to prepare and visualize social network data. Sociograms are a useful visualization tool to reveal who is interacting with whom---and, in some cases, to suggest why. In our applications of data science, we have found that the individuals (such as teachers or students) who are represented in a network often like to see what the network (and the relationships in it) *look like*. It can be compelling to think about why networks are the way they are, and how changes could be made to---for example---foster more connections between individuals who have few opportunities to interact. In this way, social network analysis can be useful to the data scientist in education because it provides a technique to communicate with other educational stakeholders in a compelling way.

Social network analysis is a broad (and growing) domain, and this chapter was intended to present some of its foundation. Fortunately for R users, many recent developments are implemented first in R (e.g., @R-amen). If you are interested in some of the additional steps that you can take to model and analyze network data, consider the appendix on two types of models (for selection and influence processes), [Appendix C](#c20c).